{
  "hash": "64414096e9650c5f006268af2e67d514",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'LAB: Linear Regression on Whiteside data'\ndate: \"2025-02-28 21:42:10.236164\"\ndraft: false \n\nexecute:\n  echo: true\n  eval: true\n  collapse: true\n\n\nformat:\n  html:\n    output-file: lab-linear-regression-whiteside.html\n  pdf:\n    output-file: lab-linear-regression-whiteside.pdf\n\n\n---\n\n\n\n\n\n\n\n\n\n\n::: {layout=\"[80,20]\"}\n\n::: {#first-column}\n\n|                            |\n|:---------------------------|\n| {{< var curriculum >}}     |\n| {{< var university >}}     |\n| Année {{< var year >}}     |\n| {{< var homepage >}}       |\n| {{< var moodle >}}         |\n\n\n::: \n\n::: {#second-column}\n![](/images/UniversiteParis_monogramme_couleur_RVB.png){align=\"right\" style=\"size:50px;\" width=75}\n:::\n\n:::\n\n\n\n\n\n\n## Introduction\n\nThe purpose of this lab is to introduce *linear regression* using base `R` and the tidyverse. \nWe work on a dataset provided by the [MASS](https://www.stats.ox.ac.uk/pub/MASS4/) package.\nThis dataset is investigated in the book by Venables and Ripley. This discusssion is  worth being read. Our aim is to relate regression as a tool for data exploration with regression as a method in statistical inference. To perform regression, we will rely on the base `R` function  `lm()` and on the eponymous S3 class `lm`. We will spend time understanding how the *formula* argument can be used to  construct a *design matrix* from a dataframe representing a dataset.  \n\n\n\n## Packages installation and loading  (again)\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We will use the following packages. \n# If needed, install them : pak::pkg_install(). \nstopifnot(\n  require(\"magrittr\"),\n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"patchwork\"), \n  require(\"gt\"),\n  require(\"glue\"),\n  require(\"skimr\"),\n  require(\"corrr\"),\n  require(\"GGally\"),\n  require(\"broom\"),\n  require(\"tidyverse\"),\n  require(ggfortify)\n)\n```\n:::\n\n\n\n\n\n\n\n\n\n\nBesides the `tidyverse`, we rely on `skimr` to perform univariate analysis, `GGally::ggpairs` to perform pairwise (bivariate) analysis. Package `corrr` provide graphical tools to explore correlation matrices.\nAt some point, we will showcase the exposing pipe `%$%` and the classical pipe `%>%` of `magrittr`. We use `gt` to display handy tables, `patchwork` to compose graphical objects. `glue` provides a kind of formatted strings. Package `broom` proves very useful when milking lienar models produced by `lm()` (and many other objects produced by estimators, tests, ...)   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Dataset\n\nThe dataset is available from package `MASS`. `MASS` can be downloaded from `cran`.\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside <- MASS::whiteside # no need to load the whole package\n\ncur_dataset <- str_to_title(as.character(substitute(whiteside)))\n# ?whiteside\n```\n:::\n\n\n\n\n\n\n\n\n\n\nThe documentation of `R` tells us a little bit more about this data set. \n\n> Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption.\n\nThis means that our sample is  made of 56 observations. Each observation corresponds to a week during heating season. For each observation. We have the average external temperature `Temp` (in degrees Celsius) and the weekly gas consumption `Gas`. We also have `Insul` which tells us whether the observation has been recorded `Before` or `After` treatment.  \n\nTemperature is the *explanatory* variable or the *covariate*. The target/response is the weekly Gas Consumption. We aim to *predict* or to *explain* the variations of weekly gas consumption as a function average weekly temperature. \n\nThe question is wether  the  treatment (insulation) modifies the relation between gas consumption and external temperature, and if we conclude that the treatment modifies the relation, in which way?.\n\nHave a glimpse at the data. \n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside %>% \n  glimpse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 56\nColumns: 3\n$ Insul <fct> Before, Before, Before, Before, Before, Before, Before, Before, ~\n$ Temp  <dbl> -0.8, -0.7, 0.4, 2.5, 2.9, 3.2, 3.6, 3.9, 4.2, 4.3, 5.4, 6.0, 6.~\n$ Gas   <dbl> 7.2, 6.9, 6.4, 6.0, 5.8, 5.8, 5.6, 4.7, 5.8, 5.2, 4.9, 4.9, 4.3,~\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nEven though the experimenter, Mr Whiteside, decided to apply a *treatment* to his house. This is not exactly what we call *experimental data*.  Namely, the experimenter has no way to clamp the external temperature. \nWith respect to the Temperature variable (the explanatory variable) we are facing *observational* data.\n\n\n\n\n# Columnwise exploration\n\n\n::: {.callout-note title=\"Question\"}\n\nBefore before proceeding to linear regressions of `Gas` with respect to `Temp`, perform univariate analysis on each variable.  \n\n- Compute summary statistics\n- Build the corresponding plots\n  \n:::\n\n::: {.content-visible when-profile='solution'} \n\n::: {.callout-tip title=\"Solution\"}\n\n\n`skimr` does the job.  There are no missing data, complete rate is always `1`, we remove non-informative \ncolumns from the output. \n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside |>\n  skimr::skim() |>\n  select(-n_missing, -complete_rate, -factor.ordered, - factor.n_unique) |>\n  gt() |>\n  gt::fmt_number(decimals=1) |>\n  gt::tab_caption(\"Whiteside dataset. Columnwise summaries\")\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lllrrrrrrrl}\n\\toprule\nskim\\_type & skim\\_variable & factor.top\\_counts & numeric.mean & numeric.sd & numeric.p0 & numeric.p25 & numeric.p50 & numeric.p75 & numeric.p100 & numeric.hist \\\\ \n\\midrule\\addlinespace[2.5pt]\nfactor & Insul & Aft: 30, Bef: 26 & NA & NA & NA & NA & NA & NA & NA & NA \\\\ \nnumeric & Temp & NA & 4.9 & 2.7 & -0.8 & 3.1 & 4.9 & 7.1 & 10.2 & ▃▅▇▇▃ \\\\ \nnumeric & Gas & NA & 4.1 & 1.2 & 1.3 & 3.5 & 4.0 & 4.6 & 7.2 & ▁▆▇▂▁ \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nAn  alternative way of doing this univariate  analysis consists in separating categorical variables from numerical ones. \n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsk <- whiteside %>% \n  skimr::skim() %>% \n  select(-n_missing, - complete_rate, -factor.ordered, - factor.n_unique)\n\nskimr::yank(sk, \"factor\") |> \n  gt() |>\n  gt::tab_caption(\"Whiteside dataset. Categorical variables summaries\")\n\n\nskimr::yank(sk, \"numeric\") |> \n  gt() |>\n  gt::fmt_number(decimals=1) |>\n  gt::tab_caption(\"Whiteside dataset. Categorical variables summaries\")\n```\n:::\n\n\n\n\n\n\n\n\n\n\nWe may test the normality of the `Gas`  and `Temp`  distribution\n\n\n\n\n\n\n\n\n\n\n::: {.cell appendix='true'}\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrl}\n\\toprule\nVar & statistic & p.value & method \\\\ \n\\midrule\\addlinespace[2.5pt]\nTemp & 0.98 & 0.48 & Shapiro-Wilk normality test \\\\ \nGas & 0.96 & 0.08 & Shapiro-Wilk normality test \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nBoth variables pass the Shapiro-Wilk test with flying colors. \n\nWe may use the Jarque-Bera test [See R bloggers on this](https://www.r-bloggers.com/2021/08/goodness-of-fit-test-jarque-bera-test-in-r/)\n\n\n\n\n\n\n\n\n\n\n::: {.cell appendix='true'}\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n```\n\n\n:::\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrrl}\n\\toprule\nVar & statistic & p.value & parameter & method \\\\ \n\\midrule\\addlinespace[2.5pt]\nTemp & 1.37 & 0.50 & 2.00 & Jarque Bera Test \\\\ \nGas & 2.74 & 0.25 & 2.00 & Jarque Bera Test \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nBoth variables also pass the Jarque-Bera test with flying colors. This is noteworthy \nsince the Jarque-Bera compares empirical skewness and kurtosis to Gaussian skewness and kurtosis. \n\n\n:::\n\n:::\n\n# Pairwise exploration\n\n::: {.callout-note title=\"Question\"}\n\nCompare distributions of numeric variables with respect to categorical variable `Insul`\n\n:::\n\n\n::: {.content-visible when-profile='solution'} \n \n::: {.callout-tip title=\"Solution\"}\n\nWe start by plotting histograms \n\nTo abide to the DRY principle,  we take \nadvantage of the fact that aesthetics and labels can be tuned incrementally. \n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_xx <- whiteside |>\n    ggplot() +\n      geom_histogram(\n        mapping=aes(fill=Insul, color=Insul, y=after_stat(density)),\n        position=\"dodge\",\n        alpha=.1,\n        bins=10) \n\np_1 <- p_xx + \n  aes(x=Temp)  +\n  theme(legend.position = \"None\")+\n  labs(title=\"External Temperature\",\n       subtitle = \"Weekly Average (Celsius)\")\n\np_2 <- p_xx + \n  aes(x=Gas)  +\n  labs(title=\"Gas Consumption\" ,\n       subtitle=\"Weekly Average\")\n\n# patchwork the two graphical objects \n(p_1 + p_2) +\n  patchwork::plot_annotation(\n    caption=\"Whiteside dataset from package MASS\"\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nNote the `position` parameter for `geom_histogram()`. Check the result for `position=\"stack\"`, `position=\"identity\"`. Make up your mind about the most convenient choice.\n\nGas Consumption distribution `After` looks shifted with respect to distribution `Before`.\n\nAnother visualization strategy consists in using the faceting mechanism\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- whiteside %>% \n  pivot_longer(cols=c(Gas, Temp),\n              names_to = \"Vars\",\n              values_to = \"Vals\") %>% \n  ggplot() +\n  aes(x=Vals)  +\n  facet_wrap(~ Insul + Vars ) + \n  xlab(\"\")\n\nr +\n  aes(y=after_stat(density)) +\n  geom_histogram(alpha=.3, fill=\"white\", color=\"black\", bins=6) +\n  ggtitle(glue(\"{cur_dataset} data\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n# Covariance and correlation between `Gas` and `Temp`\n\n\n::: {.callout-note title=\"Question\"}\n\nCompute the covariance matrix of `Gas` and `Temp`\n\n:::\n\n::::: {.content-visible when-profile=\"solution\"}  \n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_n <- whiteside %>% \n  select(where(is.numeric)) %>% \n  colMeans()\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$\nC_n = \\begin{bmatrix}\n7.56 & -2.19\\\\\n-2.19 & 1.36\n\\end{bmatrix} \\qquad \\mu_n = \\begin{bmatrix}\n4.88\\\\\n4.07\n\\end{bmatrix}\n$$\n:::\n\n:::::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the covariance matrix for Gas and Temp\nC <- whiteside |>\n  select(where(is.numeric)) |> \n  cov()\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\nComputing correlations and correlations per groups is revealing. \n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use magrittr pipe %>%  to define a pipeline function\nmy_cor <-  . %>% \n  summarize(\n    pearson= cor(Temp, Gas, method=\"pearson\"),\n    kendall=cor(Temp, Gas, method=\"kendall\"),\n    spearman=cor(Temp,Gas, method=\"spearman\")\n  ) \n\nt1 <- whiteside |>\n  group_by(Insul) |> \n  my_cor()\n\n\nt2 <- whiteside |>\n  my_cor() |>\n  mutate(Insul=\"pooled\")\n\nbind_rows(t1, t2)  |> \n    gt() |>\n    gt::fmt_number(decimals=2) |>\n    gt::tab_caption(\"Whiteside data: correlations between Gas and Temp\")\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrr}\n\\toprule\nInsul & pearson & kendall & spearman \\\\ \n\\midrule\\addlinespace[2.5pt]\nBefore & -0.97 & -0.85 & -0.96 \\\\ \nAfter & -0.90 & -0.72 & -0.86 \\\\ \npooled & -0.68 & -0.47 & -0.62 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nNote the sharp increase of all correlation coefficients we data are grouped according to the control/treatment variable (`Insul`).\n\nUse `ggpairs` from `GGally` to get a quick overview of the pairwise interactions.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside |>\n  GGally::ggpairs()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n::: {.callout-note title=\"Question\"}\n\nBuild a scatterplot of the Whiteside dataset\n\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- whiteside %>% \n  ggplot() +\n  aes(x=Temp, y=Gas) +\n  geom_point(aes(shape=Insul)) +\n  xlab(\"Average Weekly Temperature (Celsius)\") +\n  ylab(\"Average Weekly Gas Consumption 1000 cube feet\") +\n  labs(\n    ## Use list unpacking\n  )\n\np + \n  ggtitle(glue(\"{cur_dataset} dataset\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/whiteside-scatter-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nNote that the dataset looks like the stacking of two bananas corresponding to the two heating seasons. \n:::\n\n:::\n\n::: {.callout-note title=\"Question\"} \n\nBuild boxplots of `Temp` and  `Gas` versus `Insul`\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- whiteside %>% \n  ggplot() +\n  aes(x=Insul)\n\nqt <- q + \n  geom_boxplot(aes(y=Temp))\n\nqg <- q + \n  geom_boxplot(aes(y=Gas))\n\n(qt + qg) +\n  patchwork::plot_annotation(title = glue(\"{cur_dataset} dataset\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nNote the two low extremes/outliers for the Gas Consumption after Insulation.  \n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nBuild violine plots  of `Temp` and `Gas` versus `Insul`\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(q + \n  geom_violin(aes(y=Temp))) +\n(q + \n  geom_violin(aes(y=Gas))) +\n  patchwork::plot_annotation(title = glue(\"{cur_dataset} dataset\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n\n::: {.callout-note title=\"Question\"}\n\nPlot density estimates of  `Temp` and `Gas` versus `Insul`.\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr +\n  stat_density(alpha=.3 , \n               fill=\"grey\", \n               color=\"black\", \n#               bw = \"SJ\",\n               adjust = .5 ) +\n  ggtitle(glue(\"{cur_dataset} data\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-17-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n# Hand-made calculation of simple linear regression estimates for `Gas` versus `Temp`\n\n::: {.callout-note title=\"Question\"}\n\nCompute slope and intercept using elementary computations\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslope <- C[1,2] / C[1,1] # slope \n\nintercept <- whiteside %$%  # exposing pipe from magrittr\n  (mean(Gas) - slope * mean(Temp)) # intercept\n\n# with(whiteside,\n#     mean(Gas) - b * mean(Temp)) \n```\n:::\n\n\n\n\n\n\n\n\n\n\nIn simple linear regression, the slope follows from the covariance matrix \nin a straightforward way. The slope can also be expressed as the linear correlation coefficient (Pearson)  times the ration between the standard deviation of the response variable and the standard deviation of the explanatory variable. \n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nOverlay the scatterplot with the regression line.\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + \n  geom_abline(slope=slope, intercept = intercept) +\n  ggtitle(glue(\"{cur_dataset} data\"), subtitle = \"Least square regression line\")\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n# Using `lm()`\n\n`lm` stands for Linear Models. Function `lm` has a number of arguments, including: \n\n- formula\n- data\n\n\n::: {.callout-note title=\"Question\"}\n\nUse `lm()` to compute slope and intercept\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm0 <- lm(Gas ~ Temp, data = whiteside)\n```\n:::\n\n\n\n\n\n\n\n\n\n\nThe result is an object of class `lm`. \n\nThe generic function `summary()` has a method for class `lm`\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm0 %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Gas ~ Temp, data = whiteside)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6324 -0.7119 -0.2047  0.8187  1.5327 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.4862     0.2357  23.275  < 2e-16 ***\nTemp         -0.2902     0.0422  -6.876 6.55e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8606 on 54 degrees of freedom\nMultiple R-squared:  0.4668,\tAdjusted R-squared:  0.457 \nF-statistic: 47.28 on 1 and 54 DF,  p-value: 6.545e-09\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThe summary is made of four parts \n\n- The `call`. Very useful if we handle many different models (corresponding to different formulae, or different datasets)\n- A numerical summary of *residuals*\n- A commented display of the *estimated coefficients*\n- Estimate of *noise scale* (under Gaussian assumptions)\n- Squared linear correlation coefficient between response variable $Y$ (`Gas`) and predictions $\\widehat{Y}$\n- A test statistic (Fisher's statistic) for assessing null hypothesis that slope is null, and corresponding $p$-value (under Gaussian assumptions).\n\n:::\n\n:::\n\nIncluding a rough summary in a report is not always a good idea. It is easy to extract  tabular versions of the summary using functions `tidy()` and `glance()` from package `broom`. \n\nFor html output `gt::gt()` allows us to polish the final output\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\nWe can use the exposing pipe from `magrittr` (or the `with` construct from base `R`) \nto build a function that extracts the coefficients estimates, standard error, $t$-statistic and associated p-values.  \n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_lm <- . %$% (    # <1>  The lhs is meant to be of class lm\n  tidy(.)  %>%         # <2> . acts as a pronoun for magrittr pipes     \n  gt::gt() %>% \n  gt::fmt_number(decimals=2) %>% \n  gt::tab_caption(glue(\"Linear regrression. Dataset: {call$data},  Formula: {deparse(call$formula)}\"))  # <3> call is evaluated as a member of the pronoun `.`\n)\n\ntidy_lm(lm0)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrrr}\n\\toprule\nterm & estimate & std.error & statistic & p.value \\\\ \n\\midrule\\addlinespace[2.5pt]\n(Intercept) & 5.49 & 0.24 & 23.28 & 0.00 \\\\ \nTemp & -0.29 & 0.04 & -6.88 & 0.00 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n`deparse()` is an important function from base `R`. It is very helpful when trying to take advantage \nof lazy evaluation mechanisms. \n\n\nTODO: more on `glue()` \n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nFunction `glance()` extract informations that can be helpful when performing model/variable selection. \n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\nThe next chunk handles several other parts of the summary. \n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance_lm <-  . %$% (\n  glance(.) %>% \n  mutate(across(-c(p.value), \n                ~ round(.x, digits=2)),\n         p.value=signif(p.value,3)) %>% \n  gt::gt() %>% \n  gt::tab_caption(glue(\"Dataset: {call$data},  Formula: {deparse(call$formula)}\"))\n)\n\nglance_lm(lm0)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{rrrrrrrrrrrr}\n\\toprule\nr.squared & adj.r.squared & sigma & statistic & p.value & df & logLik & AIC & BIC & deviance & df.residual & nobs \\\\ \n\\midrule\\addlinespace[2.5pt]\n0.47 & 0.46 & 0.86 & 47.28 & 6.55e-09 & 1 & -70.04 & 146.07 & 152.15 & 39.99 & 54 & 56 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n- `r.squared` (and `adj.r.squared`)\n- `sigma` estimates the noise standard deviation (under homoschedastic Gaussian noise assumption)\n- statistic is the Fisher statistic used to assess the hypothesis that the slope (`Temp` coefficient) is zero. It is compared with quantiles of Fisher distribution with 1 and 54 degrees of freedom (check `pf(47.28, df1=1, df2=54, lower.tail=F)` or `qf(6.55e-09, df1=1, df2=54, lower.tail=F)`).\n\n:::\n\n:::\n\n::: {.callout-note  title=\"Question\"}\n\n`R` offers a function `confint()` that can be fed with objects of class `lm`. Explain the output of this function.\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-note title=\"Solution\"}\n\nUnder the Gaussian homoschedastic noise assumption, `confint()` \nproduces confidence intervals for estimated coefficients. \nUsing the union bound, we can derive a conservative confidence rectangle.  \n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM <- confint(lm0, level=.95)  \n\nas_tibble(M) |>\n  mutate(Coeff=rownames(M)) |>\n  relocate(Coeff) |>\n  gt::gt() |>\n  gt::fmt_number(decimals=2)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrr}\n\\toprule\nCoeff & 2.5 \\% & 97.5 \\% \\\\ \n\\midrule\\addlinespace[2.5pt]\n(Intercept) & 5.01 & 5.96 \\\\ \nTemp & -0.37 & -0.21 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nPlot a $95\\%$ confidence region for the parameters  (assuming homoschedastic Gaussian noise). \n\n\n:::\n\n\n::: {.content-visible when-profile='solution'} \n \n::: {.callout-tip title=\"Solution\"}\n\nAssuming homoschedastic Gaussian noise, the distribution of $(X^T X)^{1/2}\\frac{\\widehat{\\theta}- \\theta}{\\whidehat{\\sigma}}$\nis $\\frac{\\mathcal{N}(0, \\text{Id}_p)}{\\sqrt{\\chi^2_{n-p}/(n-p)}}$ with $p=2$ and $n=56$, where  the Gaussian vector and \nthe $\\chi^2$ variable are independent. Hence\n$$\\frac{1}{2}\\left\\Vert (X^T X)^{1/2}\\frac{\\widehat{\\theta}- \\theta}{\\whidehat{\\sigma}} \\right\\Vert^2= \\frac{(\\widehat{\\theta}-\\theta)^T X^TX (\\widehat{\\theta}-\\theta)}{2\\widehat{\\sigma}^2}$$\nis distributed accordding to a Fisher distribution with $p=2$ and $n-p=54$ degrees of freedom. Let $f_{\\alpha}$ denote the \nquantile of order $1-\\alpha$ of the Fisher distribution with $p=2$ and $n-p=54$ degrees of freedom, the following \nellipsoid is a $1-\\alpha$ confidence region:\n$$\\left\\{ \\theta : \\frac{(\\widehat{\\theta}-\\theta)^T X^TX (\\widehat{\\theta}-\\theta)}{2\\widehat{\\sigma}^2} \\leq f_{\\alpha}\\right\\}$$\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(lm0)\ncoefficients(lm0)[\"(Intercept)\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   5.486193 \n```\n\n\n:::\n\n```{.r .cell-code}\nsum(residuals(lm0)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 39.99487\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_region <- function(X){ \n\n  specdec <- eigen(t(X) %*% X)\n  angle <- acos(specdec$vectors[1,1])\n  a <- sqrt(specdec$values[1])\n  b <- sqrt(specdec$values[2])\n\n  ggforce::geom_ellipse(\n    aes(\n      x0= 0,\n      y0= 0,\n      a = a,\n      b = b,\n      angle= angle\n    )\n)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  conf_region(X) +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n::: \n\n:::\n\n# Diagnostic plots\n\nMethod `plot.lm()` of generic S3 function `plot`  from base `R` offers six diagnostic plots. By default it displays four of them. \n\n::: {.callout-tip}\n\nIn order to obtain diagnostic plots as ggplot objects, use package `ggfortify` and generic function `autoplot`. \n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nWhat are the diagnostic plots good for? \n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(lm0)  \n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-28-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-28-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-28-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-28-4.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots <- autoplot(lm0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots@plots[[1]]\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots@plots[[2]]\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-31-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots@plots[[3]]\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-32-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots@plots[[4]]\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\nThe motivation and usage of diagnostic plots is explained in detail in the book by Fox and Weisberg: *An R companion to applied regression*.\n\nIn words, the diagnostic plots serve to assess the validity of the Gaussian linear modelling assumptions on the dataset. The assumptions can be challenged for a number of reasons:\n\n- The response variable may be a function of the covariates but not a lienar one. \n- The noise may be heteroschedastic\n- The noise may be non-Gaussian\n- The noise may exhibit dependencies It is. Right. Duck, duck, duck. Duck, duck, duck. Hello. Returned Aberprey. So longage. I. Said Vaughan.  \n\n:::\n\n:::\n\nThe diagnostic plots are built from the information gathered in the `lm` object returned by `lm(...)`. \n\n{{< fa broom >}} It is convenient to extract the required pieces of information using method `augment.lm`. of *generic function* `augment()` from package `broom`. \n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm0)\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-34-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside_aug <- lm0 %>% \n  augment(whiteside)\n\nlm0 %$% ( # exposing pipe !!! \n  augment(., data=whiteside) %>% \n  mutate(across(!where(is.factor), ~ signif(.x, 3))) %>% \n  group_by(Insul) %>% \n  sample_n(5) %>% \n  ungroup() %>% \n  gt::gt() %>% \n  gt::tab_caption(glue(\"Dataset {call$data},  {deparse(call$formula)}\"))\n)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{crrrrrrrr}\n\\toprule\nInsul & Temp & Gas & .fitted & .resid & .hat & .sigma & .cooksd & .std.resid \\\\ \n\\midrule\\addlinespace[2.5pt]\nBefore & 8.0 & 4.0 & 3.16 & 0.835 & 0.0413 & 0.861 & 0.021200 & 0.992 \\\\ \nBefore & -0.8 & 7.2 & 5.72 & 1.480 & 0.0953 & 0.842 & 0.173000 & 1.810 \\\\ \nBefore & 9.1 & 3.1 & 2.85 & 0.255 & 0.0608 & 0.868 & 0.003020 & 0.305 \\\\ \nBefore & 2.5 & 6.0 & 4.76 & 1.240 & 0.0314 & 0.851 & 0.034700 & 1.460 \\\\ \nBefore & 3.9 & 4.7 & 4.35 & 0.346 & 0.0201 & 0.867 & 0.001690 & 0.406 \\\\ \nAfter & 1.5 & 4.2 & 5.05 & -0.851 & 0.0453 & 0.860 & 0.024300 & -1.010 \\\\ \nAfter & 4.9 & 3.4 & 4.06 & -0.664 & 0.0179 & 0.864 & 0.005510 & -0.779 \\\\ \nAfter & 4.6 & 3.7 & 4.15 & -0.451 & 0.0180 & 0.866 & 0.002570 & -0.529 \\\\ \nAfter & 5.3 & 3.7 & 3.95 & -0.248 & 0.0183 & 0.868 & 0.000789 & -0.291 \\\\ \nAfter & 8.7 & 2.8 & 2.96 & -0.161 & 0.0530 & 0.868 & 0.001040 & -0.193 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\nRecall that in the output of `augment()`\n\n- `.fitted`: $\\widehat{Y} = H \\times Y= X \\times \\widehat{\\beta}$ \n- `.resid`: $\\widehat{\\epsilon} = Y - \\widehat{Y}$ residuals, $\\sim (\\text{Id}_n - H) \\times \\epsilon$ \n- `.hat`: diagonal coefficients of Hat matrix $H$ \n- `.sigma`: is meant to be the estimated standard deviation of components of  $\\widehat{Y}$\n\nCompute the share of *explained variance* \n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside_aug %$% {\n  1 - (var(.resid)/(var(Gas)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4668366\n```\n\n\n:::\n\n```{.r .cell-code}\n# with(whiteside_aug,\n#   1 - (var(.resid)/(var(Gas)))\n```\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\nPlot residuals against fitted values \n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_1 <- whiteside_aug %>% \n  ggplot() +\n  aes(x=.fitted, y=.resid)+\n  geom_point(aes(shape= Insul), size=1, color=\"black\") +\n  geom_smooth(formula = y ~ x,\n              method=\"loess\",\n              se=F,\n              linetype=\"dotted\",\n              linewidth=.5,\n              color=\"black\") +\n  geom_hline(yintercept = 0, linetype=\"dashed\") +\n  xlab(\"Fitted values\") +\n  ylab(\"Residuals)\") +\n  labs(caption = \"Residuals versus Fitted\")\n```\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\nFitted against square root of standardized residuals.\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_3 <- whiteside_aug %>%\n  ggplot() +\n  aes(x=.fitted, y=sqrt(abs(.std.resid))) +\n  geom_smooth(formula = y ~ x,\n              se=F,\n              method=\"loess\",\n              linetype=\"dotted\",\n              linewidth=.5,\n              color=\"black\") +\n  xlab(\"Fitted values\") +\n  ylab(\"sqrt(|standardized residuals|)\") +\n  geom_point(aes(shape=Insul), size=1, alpha=1) +\n  labs(caption = \"Scale location\")\n```\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_2 <- whiteside_aug %>% \n  ggplot() +\n  aes(sample=.std.resid) +\n  geom_qq(size=.5, alpha=.5) +\n  stat_qq_line(linetype=\"dotted\",\n              linewidth=.5,\n              color=\"black\") +\n  coord_fixed() +\n  labs(caption=\"Residuals qqplot\") +\n  xlab(\"Theoretical quantiles\") +\n  ylab(\"Empirical quantiles of standadrdized residuals\")\n```\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\nTAF\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(diag_1 + diag_2 + diag_3 + guide_area()) + \n  plot_layout(guides=\"collect\") +\n  plot_annotation(title=glue(\"{cur_dataset} dataset\"),\n                  subtitle = glue(\"Regression diagnostic  {deparse(lm0$call$formula)}\"), caption = 'The fact that the sign of residuals depend on Insul shows that our modeling is too naive.\\n The qqplot suggests that the residuals are not collected from Gaussian homoschedastic noise.'\n                  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-40-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n# Taking into account Insulation\n\nDesign a *formula* that allows us to take into account the possible \nimpact of Insulation. Insulation may impact the relation between weekly `Gas` consumption and average external `Temperature` in two ways. Insulation may modify the `Intercept`, it may also modify the slope, that is the sensitivity of `Gas` consumption  with respect to average external `Temperature`. \n\n::: {.callout-tip}\n\nHave a look at formula documentation (`?formula`).\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1 <- lm(Gas ~ Temp * Insul, data = whiteside)\n```\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\nCheck the design using function `model.matrix()`. How can you relate this augmented design and the *one-hot encoding* of variable `Insul`?\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(lm1) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) Temp InsulAfter Temp:InsulAfter\n1           1 -0.8          0               0\n2           1 -0.7          0               0\n3           1  0.4          0               0\n4           1  2.5          0               0\n5           1  2.9          0               0\n6           1  3.2          0               0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1 %>% \n  tidy_lm()\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrrr}\n\\toprule\nterm & estimate & std.error & statistic & p.value \\\\ \n\\midrule\\addlinespace[2.5pt]\n(Intercept) & 6.85 & 0.14 & 50.41 & 0.00 \\\\ \nTemp & -0.39 & 0.02 & -17.49 & 0.00 \\\\ \nInsulAfter & -2.13 & 0.18 & -11.83 & 0.00 \\\\ \nTemp:InsulAfter & 0.12 & 0.03 & 3.59 & 0.00 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm0 |>\n  glance_lm()\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{rrrrrrrrrrrr}\n\\toprule\nr.squared & adj.r.squared & sigma & statistic & p.value & df & logLik & AIC & BIC & deviance & df.residual & nobs \\\\ \n\\midrule\\addlinespace[2.5pt]\n0.47 & 0.46 & 0.86 & 47.28 & 6.55e-09 & 1 & -70.04 & 146.07 & 152.15 & 39.99 & 54 & 56 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n\n```{.r .cell-code}\nlm1 %>% \n  glance_lm()\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{rrrrrrrrrrrr}\n\\toprule\nr.squared & adj.r.squared & sigma & statistic & p.value & df & logLik & AIC & BIC & deviance & df.residual & nobs \\\\ \n\\midrule\\addlinespace[2.5pt]\n0.93 & 0.92 & 0.32 & 222.33 & 1.23e-29 & 3 & -14.1 & 38.2 & 48.33 & 5.43 & 52 & 56 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np +\n  geom_smooth(formula='y ~ poly(x, 2)',linewidth=.5, color=\"black\",linetype=\"dashed\",  method=\"lm\", se=FALSE)+\n  aes(color=Insul) +\n  geom_smooth(aes(linetype=Insul), \n              formula='y ~ x',linewidth=.5, color=\"black\", method=\"lm\", se=FALSE) +\n  scale_color_manual(values= c(\"Before\"=\"red\", \"After\"=\"blue\")) +\n  geom_abline(intercept = 6.8538, slope=-.3932, color=\"red\") +\n  geom_abline(intercept = 6.8538 - 2.13, slope=-.3932 +.1153, color=\"blue\") + labs(\n    title=glue(\"{cur_dataset} dataset\"),\n    subtitle = glue(\"Regression: {deparse(lm1$call$formula)}\")\n    )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-45-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside_aug1 <-  augment(lm1, whiteside)\n\n(diag_1 %+% whiteside_aug1) +\n(diag_2 %+% whiteside_aug1) +\n(diag_3 %+% whiteside_aug1) +  \n guide_area() +\n  plot_layout(guides = \"collect\") +\n  plot_annotation(title=glue(\"{cur_dataset} dataset\"),\n                  subtitle = glue(\"Regression diagnostic  {deparse(lm1$call$formula)}\"), caption = 'One possible outlier.\\n Visible on all three plots.'\n                  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/lm1-diagnostics-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThe formula argument defines the design matrix and the Least-Squares problem used to estimate the coefficients.\n\n:::\n\n:::\n\n\nFunction `model.matrix()` allows us to inspect the design matrix. \n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(lm1) %>% \n  as_tibble() %>% \n  mutate(Insul=ifelse(InsulAfter,\"After\", \"Before\")) %>% \n  ungroup() %>% \n  DT::datatable(caption=glue(\"Design matrix for {deparse(lm1$call$formula)}\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/design_matrix-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(lm1)\n```\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\nIn order to solve le Least-Square problems, we have to compute \n$$(X^T \\times X)^{-1} \\times X^T$$\nThis can be done in several ways.\n\n`lm()` uses QR factorization. \n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQ <- qr.Q(lm1$qr)\nR <- qr.R(lm1$qr)  # R is upper triangular \n\nnorm(X - Q %*% R, type=\"F\") # QR Factorization\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.753321e-14\n```\n\n\n:::\n\n```{.r .cell-code}\nsignif(t(Q) %*% Q, 2)      # Q's columns form an orthonormal family\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]     [,2]     [,3]    [,4]\n[1,]  1.0e+00 -1.4e-17  3.1e-17 1.7e-16\n[2,] -1.4e-17  1.0e+00 -3.5e-17 1.4e-17\n[3,]  3.1e-17 -3.5e-17  1.0e+00 0.0e+00\n[4,]  1.7e-16  1.4e-17  0.0e+00 1.0e+00\n```\n\n\n:::\n\n```{.r .cell-code}\nH <- Q %*% t(Q)             # The Hat matrix \n\nnorm(X - H %*% X, type=\"F\") # H leaves X's columns invariant\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.758479e-14\n```\n\n\n:::\n\n```{.r .cell-code}\nnorm(H - H %*% H, type=\"F\") # H is idempotent\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.993681e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# eigen(H, symmetric = TRUE, only.values = TRUE)$values\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((solve(t(X) %*% X) %*% t(X) %*% whiteside$Gas - lm1$coefficients)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.075652e-29\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nOnce we have the QR factorization of $X$, solving the normal equations boils down to inverting a triangular matrix. \n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((solve(R) %*% t(Q) %*% whiteside$Gas - lm1$coefficients)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.050287e-29\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#matador::mat2latex(signif(solve(t(X) %*% X), 2))\n```\n:::\n\n\n\n\n\n\n\n\n\n\n$$\n(X^T \\times X)^{-1} = \\begin{bmatrix}\n0.18 & -0.026 & -0.18 & 0.026\\\\\n-0.026 & 0.0048 & 0.026 & -0.0048\\\\\n-0.18 & 0.026 & 0.31 & -0.048\\\\\n0.026 & -0.0048 & -0.048 & 0.0099\n\\end{bmatrix}\n$$\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside_aug1 %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 56\nColumns: 9\n$ Insul      <fct> Before, Before, Before, Before, Before, Before, Before, Bef~\n$ Temp       <dbl> -0.8, -0.7, 0.4, 2.5, 2.9, 3.2, 3.6, 3.9, 4.2, 4.3, 5.4, 6.~\n$ Gas        <dbl> 7.2, 6.9, 6.4, 6.0, 5.8, 5.8, 5.6, 4.7, 5.8, 5.2, 4.9, 4.9,~\n$ .fitted    <dbl> 7.168419, 7.129095, 6.696532, 5.870731, 5.713435, 5.595463,~\n$ .resid     <dbl> 0.031581243, -0.229094875, -0.296532170, 0.129269357, 0.086~\n$ .hat       <dbl> 0.22177670, 0.21586370, 0.15721835, 0.07782904, 0.06755399,~\n$ .sigma     <dbl> 0.3261170, 0.3241373, 0.3230041, 0.3256103, 0.3259138, 0.32~\n$ .cooksd    <dbl> 0.0008751645, 0.0441520664, 0.0466380672, 0.0036646607, 0.0~\n$ .std.resid <dbl> 0.11083298, -0.80096122, -1.00001423, 0.41675591, 0.2775375~\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\nUnderstanding `.fitted` column\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((predict(lm1, newdata = whiteside) - whiteside_aug1$.fitted)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\nsum((H %*% whiteside_aug1$Gas - whiteside_aug1$.fitted)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.478877e-28\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\nUnderstanding `.resid`\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((whiteside_aug1$.resid + H %*% whiteside_aug1$Gas - whiteside_aug1$Gas)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.461127e-28\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\nUnderstanding `.hat`\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((whiteside_aug1$.hat - diag(H))^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n\nUnderstanding `.std.resid` \n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_hat <- sqrt(sum(lm1$residuals^2)/lm1$df.residual)\n\nlm1 %>% glance() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.928         0.924 0.323      222. 1.23e-29     3  -14.1  38.2  48.3\n# i 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n$$\n\\widehat{r}_i = \\frac{\\widehat{\\epsilon}_i}{\\widehat{\\sigma} \\sqrt{1 - H_{i,i}}}\n$$\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((sigma_hat * sqrt(1 -whiteside_aug1$.hat) * whiteside_aug1$.std.resid - whiteside_aug1$.resid)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.471837e-28\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\nUnderstanding column `.sigma` \n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\nColumn `.sigma` contains the *leave-one-out* estimates of $\\sigma$, that is `whiteside_aug1$.sigma[i]` is the estimate of $\\sigma$ you obtain by leaving out the `i` row of the dataframe. \n\nThere is no need to recompute everything for each sample element.\n\n$$\n\\widehat{\\sigma}^2_{(i)} =  \\widehat{\\sigma}^2 \\frac{n-p-1- \\frac{\\widehat{\\epsilon}_i^2}{\\widehat{\\sigma}^2 {(1 - H_{i,i})}}\\frac{}{}}{n-p-2}\n$$\n\n:::\n\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{longtable}\n\\usepackage{colortbl}\n\\usepackage{array}\n\\usepackage{anyfontsize}\n\\usepackage{multirow}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}
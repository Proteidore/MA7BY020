{
  "hash": "ec54285145642e99ea266bd70bb05737",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'LAB: Linear Regression on Whiteside data'\ndate: \"2025-03-18 15:11:26.348787\"\ndraft: false \n\nexecute:\n  echo: true\n  eval: true\n  collapse: true\n\n\nformat:\n  html:\n    output-file: lab-linear-regression-whiteside.html\n  pdf:\n    output-file: lab-linear-regression-whiteside.pdf\n\n\n---\n\n\n\n\n::: {layout=\"[80,20]\"}\n\n::: {#first-column}\n\n|                            |\n|:---------------------------|\n| {{< var curriculum >}}     |\n| {{< var university >}}     |\n| Année {{< var year >}}     |\n| {{< var homepage >}}       |\n| {{< var moodle >}}         |\n\n\n::: \n\n::: {#second-column}\n![](/images/UniversiteParis_monogramme_couleur_RVB.png){align=\"right\" style=\"size:50px;\" width=75}\n:::\n\n:::\n\n\n\n\n\n\n## Introduction\n\nThe purpose of this lab is to introduce *linear regression* using base `R` and the tidyverse. \nWe work on a dataset provided by the [MASS](https://www.stats.ox.ac.uk/pub/MASS4/) package.\nThis dataset is investigated in the book by Venables and Ripley. This discusssion is  worth being read. Our aim is to relate regression as a tool for data exploration with regression as a method in statistical inference. To perform regression, we will rely on the base `R` function  `lm()` and on the eponymous S3 class `lm`. We will spend time understanding how the *formula* argument can be used to  construct a *design matrix* from a dataframe representing a dataset.  \n\n\n\n## Packages installation and loading  (again)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We will use the following packages. \n# If needed, install them : pak::pkg_install(). \nstopifnot(\n  require(\"magrittr\"),\n  require(\"lobstr\"),\n  require(\"ggforce\"),\n  require(\"patchwork\"), \n  require(\"gt\"),\n  require(\"glue\"),\n  require(\"skimr\"),\n  require(\"corrr\"),\n  require(\"GGally\"),\n  require(\"broom\"),\n  require(\"tidyverse\"),\n  require(\"ggfortify\"),\n  require(\"autoplotly\")\n\n)\n```\n:::\n\n\n\n\nBesides the `tidyverse`, we rely on `skimr` to perform univariate analysis, `GGally::ggpairs` to perform pairwise (bivariate) analysis. Package `corrr` provide graphical tools to explore correlation matrices.\nAt some point, we will showcase the exposing pipe `%$%` and the classical pipe `%>%` of `magrittr`. We use `gt` to display handy tables, `patchwork` to compose graphical objects. `glue` provides a kind of formatted strings. Package `broom` proves very useful when milking lienar models produced by `lm()` (and many other objects produced by estimators, tests, ...)   \n\n\n\n\n\n\n\n\n\n\n\n# Dataset\n\nThe dataset is available from package `MASS`. `MASS` can be downloaded from `cran`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside <- MASS::whiteside # no need to load the whole package\n\ncur_dataset <- str_to_title(as.character(substitute(whiteside)))\n# ?whiteside\n```\n:::\n\n\n\n\nThe documentation of `R` tells us a little bit more about this data set. \n\n> Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption.\n\nThis means that our sample is  made of 56 observations. Each observation corresponds to a week during heating season. For each observation. We have the average external temperature `Temp` (in degrees Celsius) and the weekly gas consumption `Gas`. We also have `Insul` which tells us whether the observation has been recorded `Before` or `After` treatment.  \n\nTemperature is the *explanatory* variable or the *covariate*. The target/response is the weekly Gas Consumption. We aim to *predict* or to *explain* the variations of weekly gas consumption as a function average weekly temperature. \n\nThe question is wether  the  treatment (insulation) modifies the relation between gas consumption and external temperature, and if we conclude that the treatment modifies the relation, in which way?.\n\nHave a glimpse at the data. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside %>% \n  glimpse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 56\nColumns: 3\n$ Insul <fct> Before, Before, Before, Before, Before, Before, Before, Before, ~\n$ Temp  <dbl> -0.8, -0.7, 0.4, 2.5, 2.9, 3.2, 3.6, 3.9, 4.2, 4.3, 5.4, 6.0, 6.~\n$ Gas   <dbl> 7.2, 6.9, 6.4, 6.0, 5.8, 5.8, 5.6, 4.7, 5.8, 5.2, 4.9, 4.9, 4.3,~\n```\n\n\n:::\n:::\n\n\n\n\nEven though the experimenter, Mr Whiteside, decided to apply a *treatment* to his house. This is not exactly what we call *experimental data*.  Namely, the experimenter has no way to clamp the external temperature. \nWith respect to the Temperature variable (the explanatory variable) we are facing *observational* data.\n\n\n\n\n# Columnwise exploration\n\n\n::: {.callout-note title=\"Question\"}\n\nBefore before proceeding to linear regressions of `Gas` with respect to `Temp`, perform univariate analysis on each variable.  \n\n- Compute summary statistics\n- Build the corresponding plots\n  \n:::\n\n::: {.content-visible when-profile='solution'} \n\n::: {.callout-tip title=\"Solution\"}\n\n\n`skimr` does the job.  There are no missing data, complete rate is always `1`, we remove non-informative \ncolumns from the output. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside |>\n  skimr::skim() |>\n  select(-n_missing, -complete_rate, -factor.ordered, - factor.n_unique) |>\n  gt() |>\n  gt::fmt_number(decimals=1) |>\n  gt::tab_caption(\"Whiteside dataset. Columnwise summaries\")\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lllrrrrrrrl}\n\\toprule\nskim\\_type & skim\\_variable & factor.top\\_counts & numeric.mean & numeric.sd & numeric.p0 & numeric.p25 & numeric.p50 & numeric.p75 & numeric.p100 & numeric.hist \\\\ \n\\midrule\\addlinespace[2.5pt]\nfactor & Insul & Aft: 30, Bef: 26 & NA & NA & NA & NA & NA & NA & NA & NA \\\\ \nnumeric & Temp & NA & 4.9 & 2.7 & -0.8 & 3.1 & 4.9 & 7.1 & 10.2 & ▃▅▇▇▃ \\\\ \nnumeric & Gas & NA & 4.1 & 1.2 & 1.3 & 3.5 & 4.0 & 4.6 & 7.2 & ▁▆▇▂▁ \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\nAn  alternative way of doing this univariate  analysis consists in separating categorical variables from numerical ones. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsk <- whiteside %>% \n  skimr::skim() %>% \n  select(-n_missing, - complete_rate, -factor.ordered, - factor.n_unique)\n\nskimr::yank(sk, \"factor\") |> \n  gt() |>\n  gt::tab_caption(\"Whiteside dataset. Categorical variables summaries\")\n\n\nskimr::yank(sk, \"numeric\") |> \n  gt() |>\n  gt::fmt_number(decimals=1) |>\n  gt::tab_caption(\"Whiteside dataset. Categorical variables summaries\")\n```\n:::\n\n\n\n\nWe may test the normality of the `Gas`  and `Temp`  distribution\n\n\n\n\n::: {.cell appendix='true'}\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrl}\n\\toprule\nVar & statistic & p.value & method \\\\ \n\\midrule\\addlinespace[2.5pt]\nTemp & 0.98 & 0.48 & Shapiro-Wilk normality test \\\\ \nGas & 0.96 & 0.08 & Shapiro-Wilk normality test \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\nBoth variables pass the Shapiro-Wilk test with flying colors. \n\nWe may use the Jarque-Bera test [See R bloggers on this](https://www.r-bloggers.com/2021/08/goodness-of-fit-test-jarque-bera-test-in-r/)\n\n\n\n\n::: {.cell appendix='true'}\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrrl}\n\\toprule\nVar & statistic & p.value & parameter & method \\\\ \n\\midrule\\addlinespace[2.5pt]\nTemp & 1.37 & 0.50 & 2.00 & Jarque Bera Test \\\\ \nGas & 2.74 & 0.25 & 2.00 & Jarque Bera Test \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\nBoth variables also pass the Jarque-Bera test with flying colors. This is noteworthy \nsince the Jarque-Bera compares empirical skewness and kurtosis to Gaussian skewness and kurtosis. \n\n\n:::\n\n:::\n\n# Pairwise exploration\n\n::: {.callout-note title=\"Question\"}\n\nCompare distributions of numeric variables with respect to categorical variable `Insul`\n\n:::\n\n\n::: {.content-visible when-profile='solution'} \n \n::: {.callout-tip title=\"Solution\"}\n\nWe start by plotting histograms \n\nTo abide to the DRY principle,  we take \nadvantage of the fact that aesthetics and labels can be tuned incrementally. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_xx <- whiteside |>\n    ggplot() +\n      geom_histogram(\n        mapping=aes(fill=Insul, color=Insul, y=after_stat(density)),\n        position=\"dodge\",\n        alpha=.1,\n        bins=10) \n\np_1 <- p_xx + \n  aes(x=Temp)  +\n  theme(legend.position = \"None\")+\n  labs(title=\"External Temperature\",\n       subtitle = \"Weekly Average (Celsius)\")\n\np_2 <- p_xx + \n  aes(x=Gas)  +\n  labs(title=\"Gas Consumption\" ,\n       subtitle=\"Weekly Average\")\n\n# patchwork the two graphical objects \n(p_1 + p_2) +\n  patchwork::plot_annotation(\n    caption=\"Whiteside dataset from package MASS\"\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/histo-seasons-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nNote the `position` parameter for `geom_histogram()`. Check the result for `position=\"stack\"`, `position=\"identity\"`. Make up your mind about the most convenient choice.\n\nGas Consumption distribution `After` looks shifted with respect to distribution `Before`.\n\nAnother visualization strategy consists in using the faceting mechanism\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- whiteside %>% \n  pivot_longer(cols=c(Gas, Temp),\n              names_to = \"Vars\",\n              values_to = \"Vals\") %>% \n  ggplot() +\n  aes(x=Vals)  +\n  facet_wrap(~ Insul + Vars ) + \n  xlab(\"\")\n\nr +\n  aes(y=after_stat(density)) +\n  geom_histogram(alpha=.3, fill=\"white\", color=\"black\", bins=6) +\n  ggtitle(glue(\"{cur_dataset} data\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/before-after-histo-facet-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Temp ~ Insul, data=whiteside) |>\n  broom::glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0263       0.00830  2.74      1.46   0.232     1  -135.  276.  282.\n# i 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Temp ~ Insul, data=whiteside) |>\n  anova()  |>\n  broom::tidy() |>\n  gt::gt() |>\n  gt::fmt_number(decimals = 3) |>\n  gt::tab_caption(\"Testing temperature homogeneity over the seasons\")\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrrrr}\n\\toprule\nterm & df & sumsq & meansq & statistic & p.value \\\\ \n\\midrule\\addlinespace[2.5pt]\nInsul & 1.000 & 10.950 & 10.950 & 1.461 & 0.232 \\\\ \nResiduals & 54.000 & 404.855 & 7.497 & NA & NA \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\nThe difference between temperature distributions over the two seasons is not deemed to be significant by ANOVA. \n\nIt is important that the grouping variable (here `Insul`) is a factor.\n\n\n:::\n\n:::\n\n\n# Covariance and correlation between `Gas` and `Temp`\n\n\n::: {.callout-note title=\"Question\"}\n\nCompute the covariance matrix of `Gas` and `Temp`\n\n:::\n\n::::: {.content-visible when-profile=\"solution\"}  \n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the covariance matrix for Gas and Temp\nC <- whiteside |>\n  select(where(is.numeric)) |> \n  cov()\n\nmu_n <- whiteside %>% \n  select(where(is.numeric)) %>% \n  colMeans()\n```\n:::\n\n::: {.cell caption='Covariance matrix of Gas and Temp'}\n\n```{.r .cell-code}\nmatador::mat2latex(round(C,2))\n```\n\n$$\n\\begin{bmatrix}\n7.56 & -2.19\\\\\n-2.19 & 1.36\n\\end{bmatrix}\n$$\n:::\n\n\n\n\n\n:::\n\n:::::\n\n::: {.callout-note title=\"Question\"}\n\n\n\n- Compute correlations (Pearson, Kendall, Spearman) and correlations per group\n- Comment\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use magrittr pipe %>%  to define a pipeline function\nmy_cor <-  . %>% \n  summarize(\n    pearson= cor(Temp, Gas, method=\"pearson\"),\n    kendall=cor(Temp, Gas, method=\"kendall\"),\n    spearman=cor(Temp,Gas, method=\"spearman\")\n  ) \n\n# even better: pass column names as arguments\n\nmy_cor2 <-  function(df, col1, col2){\n  df %>% \n  summarize(\n    pearson= cor({{col1}}, {{col2}}, method=\"pearson\"),\n    kendall=cor({{col1}}, {{col2}}, method=\"kendall\"),\n    spearman=cor({{col1}}, {{col2}}, method=\"spearman\")\n  ) \n}\n\nt1 <- whiteside |>\n  group_by(Insul) |> \n  my_cor()\n\n# group_by(whiteside, Insul) |> my_cor2(Gas, Temp)\n\nt2 <- whiteside |>\n  my_cor() |>\n  mutate(Insul=\"pooled\")\n\n# whiteside |> my_cor2(Gas, Temp) |> mutate(Insul=\"pooled\")\n\nbind_rows(t1, t2)  |> \n    gt() |>\n    gt::fmt_number(decimals=2) |>\n    gt::tab_caption(\"Whiteside data: correlations between Gas and Temp\")\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrr}\n\\toprule\nInsul & pearson & kendall & spearman \\\\ \n\\midrule\\addlinespace[2.5pt]\nBefore & -0.97 & -0.85 & -0.96 \\\\ \nAfter & -0.90 & -0.72 & -0.86 \\\\ \npooled & -0.68 & -0.47 & -0.62 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\nNote the sharp increase of all correlation coefficients we data are grouped according to the control/treatment variable (`Insul`).\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\n\nUse `ggpairs` from `GGally` to get a quick overview of the pairwise interactions.\n\n:::\n\n::::: {.content-visible when-profile=\"solution\"}  \n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_minimal())\n\nwhiteside |>\n  GGally::ggpairs()\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/ggally-ggpairs-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n:::\n\n:::::\n\n\n\n\n\n\n::: {.callout-note title=\"Question\"}\n\nBuild a scatterplot of the Whiteside dataset\n\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- whiteside |> \n  ggplot() +\n  aes(x=Temp, y=Gas) +\n  geom_point(aes(shape=Insul)) +\n  xlab(\"Average Weekly Temperature (Celsius)\") +\n  ylab(\"Average Weekly Gas Consumption 1000 cube feet\") +\n  labs(\n    ## Use list unpacking\n  )\n\np + \n  ggtitle(glue(\"{cur_dataset} dataset\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/whiteside-scatter-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nNote that the dataset looks like the stacking of two bananas corresponding to the two heating seasons. \n:::\n\n:::\n\n::: {.callout-note title=\"Question\"} \n\nBuild boxplots of `Temp` and  `Gas` versus `Insul`\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- whiteside %>% \n  ggplot() +\n  aes(x=Insul)\n\nqt <- q + \n  geom_boxplot(aes(y=Temp))\n\nqg <- q + \n  geom_boxplot(aes(y=Gas))\n\n(qt + qg) +\n  patchwork::plot_annotation(title = glue(\"{cur_dataset} dataset\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/boxplots-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nNote the two low extremes/outliers for the Gas Consumption after Insulation.  \n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nBuild violine plots  of `Temp` and `Gas` versus `Insul`\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(q + \n  geom_violin(aes(y=Temp))) +\n(q + \n  geom_violin(aes(y=Gas))) +\n  patchwork::plot_annotation(title = glue(\"{cur_dataset} dataset\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/violines-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\n\n::: {.callout-note title=\"Question\"}\n\nPlot density estimates of  `Temp` and `Gas` versus `Insul`.\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr +\n  stat_density(alpha=.3 , \n               fill=\"grey\", \n               color=\"black\", \n#               bw = \"SJ\",\n               adjust = .5 ) +\n  ggtitle(glue(\"{cur_dataset} data\"))\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/densities-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n# Hand-made calculation of simple linear regression estimates for `Gas` versus `Temp`\n\n::: {.callout-note title=\"Question\"}\n\nCompute slope and intercept using elementary computations\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslope <- C[1,2] / C[1,1] # slope \n\nintercept <- whiteside %$%  # exposing pipe from magrittr\n  (mean(Gas) - slope * mean(Temp)) # intercept\n\n# with(whiteside,\n#     mean(Gas) - b * mean(Temp)) \n```\n:::\n\n\n\n\nIn simple linear regression, the slope follows from the covariance matrix \nin a straightforward way. The slope can also be expressed as the linear correlation coefficient (Pearson)  times the ration between the standard deviation of the response variable and the standard deviation of the explanatory variable. \n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nOverlay the scatterplot with the regression line.\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + \n  geom_abline(slope=slope, intercept = intercept) +\n  ggtitle(glue(\"{cur_dataset} data\"), subtitle = \"Least square regression line\")\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/overlay-scatterplot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\n# Using `lm()`\n\n`lm` stands for Linear Models. Function `lm` has a number of arguments, including: \n\n- formula\n- data\n\n\n::: {.callout-note title=\"Question\"}\n\nUse `lm()` to compute slope and intercept. Denote the object created \nby constructor `lm()` as `lm0`. \n\n- What is the class of `lm0` ?\n- \n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm0 <- lm(Gas ~ Temp, data = whiteside)\n```\n:::\n\n\n\n\nThe result is an object of class `lm`. \n\nThe generic function `summary()` has a method for class `lm`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm0 %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Gas ~ Temp, data = whiteside)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6324 -0.7119 -0.2047  0.8187  1.5327 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.4862     0.2357  23.275  < 2e-16 ***\nTemp         -0.2902     0.0422  -6.876 6.55e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8606 on 54 degrees of freedom\nMultiple R-squared:  0.4668,\tAdjusted R-squared:  0.457 \nF-statistic: 47.28 on 1 and 54 DF,  p-value: 6.545e-09\n```\n\n\n:::\n:::\n\n\n\n\nThe summary is made of four parts \n\n- The `call`. Very useful if we handle many different models (corresponding to different formulae, or different datasets)\n- A numerical summary of *residuals*\n- A commented display of the *estimated coefficients*\n- Estimate of *noise scale* (under Gaussian assumptions)\n- Squared linear correlation coefficient between response variable $Y$ (`Gas`) and predictions $\\widehat{Y}$\n- A test statistic (Fisher's statistic) for assessing null hypothesis that slope is null, and corresponding $p$-value (under Gaussian assumptions).\n\n:::\n\n:::\n\nIncluding a rough summary in a report is not always a good idea. It is easy to extract  tabular versions of the summary using functions `tidy()` and `glance()` from package `broom`. \n\nFor html output `gt::gt()` allows us to polish the final output\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\nWe can use the exposing pipe from `magrittr` (or the `with` construct from base `R`) \nto build a function that extracts the coefficients estimates, standard error, $t$-statistic and associated p-values.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_lm <- . %$% (    # <1>  The lhs is meant to be of class lm\n  tidy(.)  %>%         # <2> . acts as a pronoun for magrittr pipes     \n  gt::gt() %>% \n  gt::fmt_number(decimals=2) %>% \n  gt::tab_caption(glue(\"Linear regrression. Dataset: {call$data},  Formula: {deparse(call$formula)}\"))  # <3> call is evaluated as a member of the pronoun `.`\n)\n\ntidy_lm(lm0)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrrr}\n\\toprule\nterm & estimate & std.error & statistic & p.value \\\\ \n\\midrule\\addlinespace[2.5pt]\n(Intercept) & 5.49 & 0.24 & 23.28 & 0.00 \\\\ \nTemp & -0.29 & 0.04 & -6.88 & 0.00 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n`deparse()` is an important function from base `R`. It is very helpful when trying to take advantage \nof lazy evaluation mechanisms. \n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nFunction `glance()` extract informations that can be helpful when performing model/variable selection. \n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\nThe next chunk handles several other parts of the summary. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance_lm <-  . %$% (\n  glance(.) %>% \n  mutate(across(-c(p.value), \n                ~ round(.x, digits=2)),\n         p.value=signif(p.value,3)) %>% \n  gt::gt() %>% \n  gt::tab_caption(glue(\"Dataset: {call$data},  Formula: {deparse(call$formula)}\"))\n)\n\nglance_lm(lm0)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{rrrrrrrrrrrr}\n\\toprule\nr.squared & adj.r.squared & sigma & statistic & p.value & df & logLik & AIC & BIC & deviance & df.residual & nobs \\\\ \n\\midrule\\addlinespace[2.5pt]\n0.47 & 0.46 & 0.86 & 47.28 & 6.55e-09 & 1 & -70.04 & 146.07 & 152.15 & 39.99 & 54 & 56 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n- `r.squared` (and `adj.r.squared`)\n- `sigma` estimates the noise standard deviation (under homoschedastic Gaussian noise assumption)\n- statistic is the Fisher statistic used to assess the hypothesis that the slope (`Temp` coefficient) is zero. It is compared with quantiles of Fisher distribution with 1 and 54 degrees of freedom (check `pf(47.28, df1=1, df2=54, lower.tail=F)` or `qf(6.55e-09, df1=1, df2=54, lower.tail=F)`).\n\n:::\n\n:::\n\n::: {.callout-note  title=\"Question\"}\n\n`R` offers a function `confint()` that can be fed with objects of class `lm`. Explain the output of this function.\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-note title=\"Solution\"}\n\nUnder the Gaussian homoschedastic noise assumption, `confint()` \nproduces confidence intervals for estimated coefficients. \nUsing the union bound, we can derive a conservative confidence rectangle.  \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM <- confint(lm0, level=.95)  \n\nas_tibble(M) |>\n  mutate(Coeff=rownames(M)) |>\n  relocate(Coeff) |>\n  gt::gt() |>\n  gt::fmt_number(decimals=2)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrr}\n\\toprule\nCoeff & 2.5 \\% & 97.5 \\% \\\\ \n\\midrule\\addlinespace[2.5pt]\n(Intercept) & 5.01 & 5.96 \\\\ \nTemp & -0.37 & -0.21 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_levels <- c(.90, .95, .99)\n\npurrr::map(conf_levels, \\(x) confint(lm0, level=x)) |>\n  purrr::map(as_tibble) |>\n  purrr::map(\\(x) mutate(x, coeffs=c(\"Intercept\", \"Temp\")))|>\n  purrr::map(\\(x) pivot_longer(x, cols=ends_with(\"%\"), names_to=\"conf\", values_to=\"bound\")) |>\n  bind_rows() |>\n  gt::gt() |>\n  gt::fmt_number(decimals = 2)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrr}\n\\toprule\ncoeffs & conf & bound \\\\ \n\\midrule\\addlinespace[2.5pt]\nIntercept & 5 \\% & 5.09 \\\\ \nIntercept & 95 \\% & 5.88 \\\\ \nTemp & 5 \\% & -0.36 \\\\ \nTemp & 95 \\% & -0.22 \\\\ \nIntercept & 2.5 \\% & 5.01 \\\\ \nIntercept & 97.5 \\% & 5.96 \\\\ \nTemp & 2.5 \\% & -0.37 \\\\ \nTemp & 97.5 \\% & -0.21 \\\\ \nIntercept & 0.5 \\% & 4.86 \\\\ \nIntercept & 99.5 \\% & 6.12 \\\\ \nTemp & 0.5 \\% & -0.40 \\\\ \nTemp & 99.5 \\% & -0.18 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nPlot a $95\\%$ confidence region for the parameters  (assuming homoschedastic Gaussian noise). \n\n\n:::\n\n\n::: {.content-visible when-profile='solution'} \n \n::: {.callout-tip title=\"Solution\"}\n\nAssuming homoschedastic Gaussian noise, the distribution of $(X^T X)^{1/2}\\frac{\\widehat{\\theta}- \\theta}{\\widehat{\\sigma}}$\nis $\\frac{\\mathcal{N}(0, \\text{Id}_p)}{\\sqrt{\\chi^2_{n-p}/(n-p)}}$ with $p=2$ and $n=56$, where  the Gaussian vector and \nthe $\\chi^2$ variable are independent. Hence\n$$\\frac{1}{2}\\left\\Vert (X^T X)^{1/2}\\frac{\\widehat{\\theta}- \\theta}{\\widehat{\\sigma}} \\right\\Vert^2= \\frac{(\\widehat{\\theta}-\\theta)^T X^TX (\\widehat{\\theta}-\\theta)}{2\\widehat{\\sigma}^2}$$\nis distributed accordding to a Fisher distribution with $p=2$ and $n-p=54$ degrees of freedom. Let $f_{\\alpha}$ denote the \nquantile of order $1-\\alpha$ of the Fisher distribution with $p=2$ and $n-p=54$ degrees of freedom, the following \nellipsoid is a $1-\\alpha$ confidence region:\n$$\\left\\{ \\theta : \\frac{(\\widehat{\\theta}-\\theta)^T X^TX (\\widehat{\\theta}-\\theta)}{2\\widehat{\\sigma}^2} \\leq f_{\\alpha}\\right\\}$$\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(lm0)\ncoefficients(lm0)[\"(Intercept)\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   5.486193 \n```\n\n\n:::\n\n```{.r .cell-code}\nsum(residuals(lm0)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 39.99487\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_region <- function(X, x0=0, y0=0, sigma=1, df1=2, df2=nrow(X)-2, alpha=.05, ...){ \n\n  falpha <- qf(alpha, df1, df2, lower.tail = F)\n\n  specdec <- eigen(t(X) %*% X)\n  angle <- acos(specdec$vectors[1,1])\n  a <- sigma * sqrt(2*falpha/specdec$values[1])\n  b <- sigma * sqrt(2*falpha/specdec$values[2])\n\n  \n\n  ggforce::geom_ellipse(\n    aes(\n      x0= x0,\n      y0= y0,\n      a = a,\n      b = b,\n      angle= angle\n    ),\n    ...\n)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(zeallot)  # experimental \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: zeallot\n```\n\n\n:::\n\n```{.r .cell-code}\nc(x0,y0) %<-% coefficients(lm0)\n\nggplot() +\n  conf_region(X, x0, y0, linetype='dashed') +\n  conf_region(X, x0, y0, alpha=.01, linetype='dotted') +\n  conf_region(X, x0, y0, alpha=.1, )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/plot-confidence-region-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n  coord_fixed()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ggproto object: Class CoordFixed, CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    ratio: 1\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordFixed, CoordCartesian, Coord, gg>\n```\n\n\n:::\n:::\n\n\n\n\n::: \n\n:::\n\n# Diagnostic plots\n\nMethod `plot.lm()` of generic S3 function `plot`  from base `R` offers six diagnostic plots. By default it displays four of them. \n\n::: {.callout-tip}\n\nIn order to obtain diagnostic plots as ggplot objects, use package `ggfortify` which defines an S3 method for class 'lm' for generic function  `autoplot` (defined \nin package `ggplot`). \n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nWhat are the diagnostic plots good for? \n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\nDiagnostic plots for linear regression serve several purposes:\n\n- Visualization of  possible dependencies between residuals and fitted values. OLS theory tells us that residuals and fitted values are (empirically) linearly uncorrelated. \nThis does not preclude other kinds of dependencies. \n- With some overplotting, it is also possible to visualize possible dependencies between residuals and variables that were not taken into account while computing OLS estimates. \n- Assesment of possible heteroschedasticity, for example, dependence of noise variance on fitted values.\n- Assessing departure of noise distribution from Gaussian setting.\n- Spotting points with high leverage\n- Spotting possible outliers. \n\n::: {.callout-note title=\"Question\"}\n\nLoad package `ggfortify`, and call `autoplot()` (from `ggplot2`) to build the diagnostic plots for `lm0`. \n\nGeneric function `autoplot()` called on  on an object of class `lm` builds a collection of grapical objects that \nparallel the output of base `R`  generic `plot()` function. The graphical objects output by `autoplot()` can be \nfurther tweaked as any `ggplot` object.   \n\n:::\n\n\n::: {.content-visible when-profile='solution'} \n \n \n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvanilla <- c(\n  \"title\"= \"Diagnostic plot for linear regression  Gas ~ Temp\",\n  \"caption\"= \"Whiteside data from package MASS\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots <- autoplot(lm0, data=whiteside)\n```\n:::\n\n\n\n\n`diag_plots` is an instance of an [`S4`  class]() \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(diag_plots)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ggmultiplot\"\nattr(,\"package\")\n[1] \"ggfortify\"\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots@plots[[1]] +\n  labs(!!!vanilla)\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-35-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nThis diagnostic plots would be much more helpful if we could map `Insul` on the `shape` aesthetics. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# diag_plots@plots[[1]]  + aes(shape=Insul)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm0_augmented <- augment(lm0, data=whiteside) \n\nlm0_augmented |>\n  ggplot() +\n  aes(x=.fitted, y=.resid) +\n  geom_point(aes(shape=Insul)) +\n  geom_smooth(formula='y~x', \n              method=\"loess\", \n              se=T, \n              color=\"black\",\n              linewidth=.5) +\n  xlab(\"Fitted values\") +\n  ylab(\"Residuals\") +\n  labs(\n    !!!vanilla,\n    subtitle = \"Residuals versus fitted values\"\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-37-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nOverplotting (mapping `Insul` over shape aesthetics) reveals that residuals \ndo depend on `Insul`: residual signs are a function of `Insul`. \n\nThe *smooth* regression line (blue line) departs from the `x` axis, but not in a striking way.  \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npatchwork::wrap_plots(\n  (diag_plots@plots[[2]] + coord_fixed() + labs()),  \n  (\n  augment(lm0, whiteside) |>\n  ggplot() +\n    aes(y=sort(.std.resid), \n        x=qnorm(seq_along(.std.resid)/length(.std.resid))) +\n    geom_point(aes(shape=Insul)) + \n    geom_abline(intercept = 0, slope=1.0111398) +\n    geom_smooth(method=\"lm\", se=F, linetype=\"dotted\") +\n    geom_abline(intercept=0, slope=4/3) +\n    xlab(\"Normal quantiles\") +\n    ylab(\"Standardized residuals\") +\n    coord_fixed() +\n    labs()\n  )\n) +\npatchwork::plot_annotation(\n  title =vanilla[\"title\"],\n  subtitle=\"QQ plots for linear regression\",\n  caption = vanilla[\"caption\"]\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/patch-diag-plots-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots@plots[[3]] +\n  geom_smooth(method=\"lm\", se=T, formula='y ~ x', linetype=\"dotted\") +\n  labs(\n    !!!vanilla,\n    subtitle = \"Standardized residuals versus fitted values\"\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-39-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nThe amplitude of *standardized residuals* seems to be an increasing function of \nfitted values. This suggests departure from homoschedasticity. \n\nTwo points are singled out (with row index)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots@plots[[4]] +\n  \n  labs(\n    !!!vanilla,\n    subtitle = \"\"\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-40-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nDouble check points with high leverage.\n\nWhy points with high leverage and large standardized residuals matter. \n\n\n\nThe motivation and usage of diagnostic plots is explained in detail in the book by Fox and Weisberg: *An R companion to applied regression*.\n\nIn words, the diagnostic plots serve to assess the validity of the Gaussian linear modelling assumptions on the dataset. The assumptions can be challenged for a number of reasons:\n\n- The response variable may be a function of the covariates but not a lienar one. \n- The noise may be heteroschedastic\n- The noise may be non-Gaussian\n- The noise may exhibit dependencies.  \n\n:::\n\n:::\n\nThe diagnostic plots are built from the information gathered in the `lm` object returned by `lm(...)`. \n\n{{< fa broom >}} It is convenient to extract the required pieces of information using method `augment.lm`. of *generic function* `augment()` from package `broom`. \n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm0)\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-41-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside_aug <- lm0 %>% \n  augment(whiteside)\n\nlm0 %$% ( # exposing pipe !!! \n  augment(., data=whiteside) %>% \n  mutate(across(!where(is.factor), ~ signif(.x, 3))) %>% \n  group_by(Insul) %>% \n  sample_n(5) %>% \n  ungroup() %>% \n  gt::gt() %>% \n  gt::tab_caption(glue(\"Dataset {call$data},  {deparse(call$formula)}\"))\n)\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{crrrrrrrr}\n\\toprule\nInsul & Temp & Gas & .fitted & .resid & .hat & .sigma & .cooksd & .std.resid \\\\ \n\\midrule\\addlinespace[2.5pt]\nBefore & 8.0 & 4.0 & 3.16 & 0.835 & 0.0413 & 0.861 & 0.021200 & 0.992 \\\\ \nBefore & -0.8 & 7.2 & 5.72 & 1.480 & 0.0953 & 0.842 & 0.173000 & 1.810 \\\\ \nBefore & 9.1 & 3.1 & 2.85 & 0.255 & 0.0608 & 0.868 & 0.003020 & 0.305 \\\\ \nBefore & 2.5 & 6.0 & 4.76 & 1.240 & 0.0314 & 0.851 & 0.034700 & 1.460 \\\\ \nBefore & 3.9 & 4.7 & 4.35 & 0.346 & 0.0201 & 0.867 & 0.001690 & 0.406 \\\\ \nAfter & 1.5 & 4.2 & 5.05 & -0.851 & 0.0453 & 0.860 & 0.024300 & -1.010 \\\\ \nAfter & 4.9 & 3.4 & 4.06 & -0.664 & 0.0179 & 0.864 & 0.005510 & -0.779 \\\\ \nAfter & 4.6 & 3.7 & 4.15 & -0.451 & 0.0180 & 0.866 & 0.002570 & -0.529 \\\\ \nAfter & 5.3 & 3.7 & 3.95 & -0.248 & 0.0183 & 0.868 & 0.000789 & -0.291 \\\\ \nAfter & 8.7 & 2.8 & 2.96 & -0.161 & 0.0530 & 0.868 & 0.001040 & -0.193 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\nRecall that in the output of `augment()`\n\n- `.fitted`: $\\widehat{Y} = H \\times Y= X \\times \\widehat{\\beta}$ \n- `.resid`: $\\widehat{\\epsilon} = Y - \\widehat{Y}$ residuals, $\\sim (\\text{Id}_n - H) \\times \\epsilon$ \n- `.hat`: diagonal coefficients of Hat matrix $H$ \n- `.sigma`: is meant to be the estimated standard deviation of components of  $\\widehat{Y}$\n\nCompute the share of *explained variance* \n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside_aug %$% {\n  1 - (var(.resid)/(var(Gas)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4668366\n```\n\n\n:::\n\n```{.r .cell-code}\n# with(whiteside_aug,\n#   1 - (var(.resid)/(var(Gas)))\n```\n:::\n\n\n\n\n:::\n\n:::\n\nPlot residuals against fitted values \n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_1 <- whiteside_aug %>% \n  ggplot() +\n  aes(x=.fitted, y=.resid)+\n  geom_point(aes(shape= Insul), size=1, color=\"black\") +\n  geom_smooth(formula = y ~ x,\n              method=\"loess\",\n              se=F,\n              linetype=\"dotted\",\n              linewidth=.5,\n              color=\"black\") +\n  geom_hline(yintercept = 0, linetype=\"dashed\") +\n  xlab(\"Fitted values\") +\n  ylab(\"Residuals)\") +\n  labs(caption = \"Residuals versus Fitted\")\n```\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nFitted against square root of standardized residuals.\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_3 <- whiteside_aug %>%\n  ggplot() +\n  aes(x=.fitted, y=sqrt(abs(.std.resid))) +\n  geom_smooth(formula = y ~ x,\n              se=F,\n              method=\"loess\",\n              linetype=\"dotted\",\n              linewidth=.5,\n              color=\"black\") +\n  xlab(\"Fitted values\") +\n  ylab(\"sqrt(|standardized residuals|)\") +\n  geom_point(aes(shape=Insul), size=1, alpha=1) +\n  labs(caption = \"Scale location\")\n```\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nHand-made normal qqplot for `lm0` \n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_2 <- whiteside_aug %>% \n  ggplot() +\n  aes(sample=.std.resid) +\n  geom_qq(size=.5, alpha=.5) +\n  stat_qq_line(linetype=\"dotted\",\n              linewidth=.5,\n              color=\"black\") +\n#  coord_fixed() +\n  labs(caption=\"Residuals qqplot\") +\n  xlab(\"Theoretical quantiles\") +\n  ylab(\"Empirical quantiles of standadrdized residuals\")\n```\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\n\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(diag_1 + diag_2 + diag_3 + guide_area()) + \n  plot_layout(guides=\"collect\") +\n  plot_annotation(title=glue(\"{cur_dataset} dataset\"),\n                  subtitle = glue(\"Regression diagnostic  {deparse(lm0$call$formula)}\"), caption = 'The fact that the sign of residuals depend on Insul shows that our modeling is too naive.\\n The qqplot suggests that the residuals are not collected from Gaussian homoschedastic noise.'\n                  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-47-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\n# Taking into account Insulation\n\n::: {.callout-note title=\"Question\"}\n\nDesign a *formula* that allows us to take into account the possible \nimpact of Insulation. Insulation may impact the relation between weekly `Gas` consumption and average external `Temperature` in two ways. Insulation may modify the `Intercept`, it may also modify the slope, that is the sensitivity of `Gas` consumption  with respect to average external `Temperature`. \n\n:::\n\n\n::: {.callout-tip}\n\nHave a look at formula documentation (`?formula`).\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1 <- lm(Gas ~ Temp * Insul, data = whiteside)\n\nvanilla_lm1 <- vanilla\nvanilla_lm1[\"title\"] <- \"Diagnostic plot for linear regression  Gas ~ Temp*Insul\"\n```\n:::\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nCheck the design using function `model.matrix()`. How can you relate this augmented design and the *one-hot encoding* of variable `Insul`?\n\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(lm1) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) Temp InsulAfter Temp:InsulAfter\n1           1 -0.8          0               0\n2           1 -0.7          0               0\n3           1  0.4          0               0\n4           1  2.5          0               0\n5           1  2.9          0               0\n6           1  3.2          0               0\n```\n\n\n:::\n:::\n\n\n\n\n`InsulAfter` is the result of *one-hot encoding* of boolean column `Insul`: \n`Before` is encoded by `0` and `After` is encoded by `1`. \n\n`Temp:InsulAfter` is the elementwise product of `Temp`  and `InsulAfter`. \n\nIn *one-hot-encoding*, a categorical columns with `k` levels is mapped to `k` `0-1` valued \ncolumns. An occurrence of level `j : 1 ≤ j ≤ k` is mapped to a sequence of `j-1` `0`'s, a `1`, followed by `k-j` `0`'s.   \nThe resulting matrix has column rank at most 'k-1', since the rowsums are all equal to `1`. \nIn order to eliminate this redundancy, one of the `k` columns is dropped. \nHere the column with `1`'s for `Before`  has been dropped. \n\nNote that other encodings (called *contrasts* in the statistics literature) are possible.\n\n \n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nDisplay and comment the part of the summary of `lm1` concerning estimated coefficients.\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1 %>% \n  tidy_lm()\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrrr}\n\\toprule\nterm & estimate & std.error & statistic & p.value \\\\ \n\\midrule\\addlinespace[2.5pt]\n(Intercept) & 6.85 & 0.14 & 50.41 & 0.00 \\\\ \nTemp & -0.39 & 0.02 & -17.49 & 0.00 \\\\ \nInsulAfter & -2.13 & 0.18 & -11.83 & 0.00 \\\\ \nTemp:InsulAfter & 0.12 & 0.03 & 3.59 & 0.00 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(\n  lm0 |>\n    glance() |>\n    mutate(model=\"Gas ~ Temp\"),\n  lm1 |>\n    glance() |>\n    mutate(model=\"Gas ~ Temp*Insul\")\n) |>\n  relocate(model) |>\n  gt::gt() |>\n  gt::fmt_number(decimals=2) |>\n  gt::tab_caption(\"Comparing two models for Whiteside dataset\")\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{lrrrrrrrrrrrr}\n\\toprule\nmodel & r.squared & adj.r.squared & sigma & statistic & p.value & df & logLik & AIC & BIC & deviance & df.residual & nobs \\\\ \n\\midrule\\addlinespace[2.5pt]\nGas \\textasciitilde{} Temp & 0.47 & 0.46 & 0.86 & 47.28 & 0.00 & 1.00 & -70.04 & 146.07 & 152.15 & 39.99 & 54.00 & 56.00 \\\\ \nGas \\textasciitilde{} Temp*Insul & 0.93 & 0.92 & 0.32 & 222.33 & 0.00 & 3.00 & -14.10 & 38.20 & 48.33 & 5.43 & 52.00 & 56.00 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nComment the diagnostic plots built from the extended model using `autoplot()`. If possible, \ngenerate alternative diagnostic plots pipelining `broom`  and `ggplot2`. \n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots_lm1 <- autoplot(lm1, data=whiteside)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots_lm1@plots[[1]] +\n  labs(\n    !!!vanilla_lm1,\n    subtitle=\"Residuals versus fitted values\"\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/lm1-residuals-fitted-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside_aug1 <-  augment(lm1, whiteside)\n\nwhiteside_aug1 |>\n  ggplot() + \n  aes(x=.fitted, y=.resid) +\n  geom_point(aes(shape=Insul)) +\n  geom_smooth(method=\"loess\", formula='y ~ x') +\n  labs(\n    !!!vanilla_lm1,\n    subtitle=\"Residuals versus fitted values\"\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/lm1-residuals-fitted-hand-made-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nThe sign of residuals does  not look like a function of `Insul` anymore. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np +\n  geom_smooth(formula='y ~ poly(x, 2)',linewidth=.5, color=\"black\",linetype=\"dashed\",  method=\"lm\", se=FALSE)+\n  aes(color=Insul) +\n  geom_smooth(aes(linetype=Insul), \n              formula='y ~ x',linewidth=.5, color=\"black\", method=\"lm\", se=FALSE) +\n  scale_color_manual(values= c(\"Before\"=\"red\", \"After\"=\"blue\")) +\n  geom_abline(intercept = 6.8538, slope=-.3932, color=\"red\") +\n  geom_abline(intercept = 6.8538 - 2.13, slope=-.3932 +.1153, color=\"blue\") + labs(\n    title=glue(\"{cur_dataset} dataset\"),\n    subtitle = glue(\"Regression: {deparse(lm1$call$formula)}\")\n    )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/tweaking-p-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(diag_1 %+% whiteside_aug1) +\n(diag_3 %+% whiteside_aug1) +  \n(diag_2 %+% whiteside_aug1) +\n  guide_area() +\n  plot_layout(guides = \"collect\",) +\n  plot_annotation(title=glue(\"{cur_dataset} dataset\"),\n                  subtitle = glue(\"Regression diagnostic  {deparse(lm1$call$formula)}\"), caption = 'One possible outlier.\\n Visible on all three plots.'\n                  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/diag-plots-lm1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\nThe formula argument defines the design matrix and the Least-Squares problem used to estimate the coefficients.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncowplot::plot_grid(\n  (diag_1 %+% whiteside_aug1),\n  (diag_3 %+% whiteside_aug1),\n  (diag_2 %+% whiteside_aug1),\n  align=\"none\",\n  nrow=3, \n  labels=c('A', 'B', 'C')\n)\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-56-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots_lm1@plots[[1]] + \n  diag_plots_lm1@plots[[3]] +\n  patchwork::plot_annotation(\n    title=\"Gas ~ Temp * Insul\",\n    # subtitle = \"Residuals versus fitted\",\n    caption=\"Whiteside data\"\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-57-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots_lm1@plots[[2]] +\n  labs(!!!vanilla_lm1)\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-58-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots@plots[[4]] +\n  labs(\n    !!! vanilla_lm1\n  )\n```\n\n::: {.cell-output-display}\n![](lab-linear-regression-whiteside_files/figure-pdf/unnamed-chunk-59-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\nFunction `model.matrix()` allows us to inspect the design matrix. \n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(lm1) %>% \n  as_tibble() %>% \n  mutate(Insul=ifelse(InsulAfter,\"After\", \"Before\")) %>% \n  ungroup() %>% \n  slice_sample(n=5) |>\n  gt::gt() |>\n  gt::tab_caption(glue(\"Design matrix for {deparse(lm1$call$formula)}\")) \n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{rrrrl}\n\\toprule\n(Intercept) & Temp & InsulAfter & Temp:InsulAfter & Insul \\\\ \n\\midrule\\addlinespace[2.5pt]\n1 & 8.0 & 1 & 8.0 & After \\\\ \n1 & 8.0 & 0 & 0.0 & Before \\\\ \n1 & 4.3 & 1 & 4.3 & After \\\\ \n1 & 3.9 & 1 & 3.9 & After \\\\ \n1 & 8.5 & 0 & 0.0 & Before \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(lm1)\n```\n:::\n\n\n\n\n:::\n\n:::\n\n\nIn order to solve le Least-Square problems, we have to compute \n$$(X^T \\times X)^{-1} \\times X^T$$\nThis can be done in several ways.\n\n`lm()` uses QR factorization. \n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQ <- qr.Q(lm1$qr)\nR <- qr.R(lm1$qr)  # R is upper triangular \n\nnorm(X - Q %*% R, type=\"F\") # QR Factorization\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.753321e-14\n```\n\n\n:::\n\n```{.r .cell-code}\nsignif(t(Q) %*% Q, 2)      # Q's columns form an orthonormal family\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]     [,2]     [,3]    [,4]\n[1,]  1.0e+00 -1.4e-17  3.1e-17 1.7e-16\n[2,] -1.4e-17  1.0e+00 -3.5e-17 1.4e-17\n[3,]  3.1e-17 -3.5e-17  1.0e+00 0.0e+00\n[4,]  1.7e-16  1.4e-17  0.0e+00 1.0e+00\n```\n\n\n:::\n\n```{.r .cell-code}\nH <- Q %*% t(Q)             # The Hat matrix \n\nnorm(X - H %*% X, type=\"F\") # H leaves X's columns invariant\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.758479e-14\n```\n\n\n:::\n\n```{.r .cell-code}\nnorm(H - H %*% H, type=\"F\") # H is idempotent\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.993681e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# eigen(H, symmetric = TRUE, only.values = TRUE)$values\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((solve(t(X) %*% X) %*% t(X) %*% whiteside$Gas - lm1$coefficients)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.075652e-29\n```\n\n\n:::\n:::\n\n\n\n\nOnce we have the QR factorization of $X$, solving the normal equations boils down to inverting a triangular matrix. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((solve(R) %*% t(Q) %*% whiteside$Gas - lm1$coefficients)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.050287e-29\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#matador::mat2latex(signif(solve(t(X) %*% X), 2))\n```\n:::\n\n\n\n\n$$\n(X^T \\times X)^{-1} = \\begin{bmatrix}\n0.18 & -0.026 & -0.18 & 0.026\\\\\n-0.026 & 0.0048 & 0.026 & -0.0048\\\\\n-0.18 & 0.026 & 0.31 & -0.048\\\\\n0.026 & -0.0048 & -0.048 & 0.0099\n\\end{bmatrix}\n$$\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhiteside_aug1 %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 56\nColumns: 9\n$ Insul      <fct> Before, Before, Before, Before, Before, Before, Before, Bef~\n$ Temp       <dbl> -0.8, -0.7, 0.4, 2.5, 2.9, 3.2, 3.6, 3.9, 4.2, 4.3, 5.4, 6.~\n$ Gas        <dbl> 7.2, 6.9, 6.4, 6.0, 5.8, 5.8, 5.6, 4.7, 5.8, 5.2, 4.9, 4.9,~\n$ .fitted    <dbl> 7.168419, 7.129095, 6.696532, 5.870731, 5.713435, 5.595463,~\n$ .resid     <dbl> 0.031581243, -0.229094875, -0.296532170, 0.129269357, 0.086~\n$ .hat       <dbl> 0.22177670, 0.21586370, 0.15721835, 0.07782904, 0.06755399,~\n$ .sigma     <dbl> 0.3261170, 0.3241373, 0.3230041, 0.3256103, 0.3259138, 0.32~\n$ .cooksd    <dbl> 0.0008751645, 0.0441520664, 0.0466380672, 0.0036646607, 0.0~\n$ .std.resid <dbl> 0.11083298, -0.80096122, -1.00001423, 0.41675591, 0.2775375~\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\nUnderstanding `.fitted` column\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((predict(lm1, newdata = whiteside) - whiteside_aug1$.fitted)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\nsum((H %*% whiteside_aug1$Gas - whiteside_aug1$.fitted)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.478877e-28\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nTry understanding the computation of `.resid` in an `lm` object. \nCompare `.resid` with the projection of `Gas` on the linear subspace orthogonal \nto the columns of the design matrix. \n\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((whiteside_aug1$.resid + H %*% whiteside_aug1$Gas - whiteside_aug1$Gas)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.461127e-28\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nUnderstanding `.hat`\n\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((whiteside_aug1$.hat - diag(H))^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\n::: {.callout-note title=\"Question\"}\n\nUnderstanding `.std.resid` \n\n- Estimate noise intensity from `residuals`\n- Compare with the output of `glance()`\n\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_hat <- sqrt(sum(lm1$residuals^2)/lm1$df.residual)\n\nlm1 |>\n  glance() |>\n  gt::gt() |>\n  gt::fmt_number(decimals=2) |>\n  gt::tab_caption(glue(\"The hand-made estimate of sigma is {round(sigma_hat,2)}\"))\n```\n\n::: {.cell-output-display}\n\\begingroup\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{longtable}{rrrrrrrrrrrr}\n\\toprule\nr.squared & adj.r.squared & sigma & statistic & p.value & df & logLik & AIC & BIC & deviance & df.residual & nobs \\\\ \n\\midrule\\addlinespace[2.5pt]\n0.93 & 0.92 & 0.32 & 222.33 & 0.00 & 3.00 & -14.10 & 38.20 & 48.33 & 5.43 & 52.00 & 56.00 \\\\ \n\\bottomrule\n\\end{longtable}\n\\endgroup\n\n:::\n:::\n\n\n\n\n$$\n\\widehat{r}_i = \\frac{\\widehat{\\epsilon}_i}{\\widehat{\\sigma} \\sqrt{1 - H_{i,i}}}\n$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((sigma_hat * sqrt(1 -whiteside_aug1$.hat) * whiteside_aug1$.std.resid - whiteside_aug1$.resid)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.471837e-28\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n:::\n\n::: {.callout-note title=\"Question\"}\n\nUnderstanding column `.sigma` \n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.callout-tip title=\"Solution\"}\n\n\nColumn `.sigma` contains the *leave-one-out* estimates of $\\sigma$, that is `whiteside_aug1$.sigma[i]` is the estimate of $\\sigma$ you obtain by leaving out the `i` row of the dataframe. \n\nThere is no need to recompute everything for each sample element.\n\n$$\n\\widehat{\\sigma}^2_{(i)} =  \\widehat{\\sigma}^2 \\frac{n-p-1- \\frac{\\widehat{\\epsilon}_i^2}{\\widehat{\\sigma}^2 {(1 - H_{i,i})}}\\frac{}{}}{n-p-2}\n$$\n\n:::\n\n:::\n\n\n# Appendix\n\n## `S3`  classes in `R`\n\n\n\n\n## Generic functions for `S3`  classes\n\n`methods(autoplot)` lists the `S3` classes for which an  autoplot method is defined. \nSome methods are defined in `ggplot2`, others like `autoplot.lm` are defined in extension \npackages like `ggfortify`. \n\n\n\n## `S4` classes in `R`\n\nThe output of `autoplot.lm` is an instance  of `S4` class\n\n\n## `tibbles` with list columns\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{longtable}\n\\usepackage{colortbl}\n\\usepackage{array}\n\\usepackage{anyfontsize}\n\\usepackage{multirow}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}
{
  "hash": "292d25f75ac54496ca1ac233f44abd0b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Modeling\"\nengine: knitr\ndate: \"2025-01-14 21:20:01.185184\"\n\nexecute:\n  echo: true\n  eval: false\n  collapse: true\n\nformat:\n  html:\n    output-file: hmw-glm-2024.html\n  pdf:\n    output-file: hmw-glm-2024.pdf\n\ndraft: true\nstandalone: true\nprefer-html: true\n---\n\n\n\n\n\n\n\n\n### {{< fa map >}} Objectives\n\n\nThis homework is concerned with Gaussian Linear Models. The objective consists of working with simulated data and visualizing/illustrating the main constructions and theorems from the sections of the Statistical Inference course dedicated to Gaussian Linear Models. \n\nWe start from the `whiteside` dataset from `MASS` package (`R`). \n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nFit a linear model with formula `Gas ~ poly(Temp, degree=2, raw=T) * Insul`  to the `whiteside` data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm2 <- lm(Gas ~ poly(Temp, degree=2, raw=T) * Insul, \n          data=whiteside)\n```\n:::\n\n\n\n\n- Extract the coefficients vector ($\\widehat{\\beta}$) and the model matrix ($X$). \n- Extract the estimate $\\widehat{\\sigma}$ of Gaussian noise standard deviation from the model.  \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nYou may rename the components of $\\beta$ and the columns of $X$ up to your convenience. \n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 Int           6.76      0.151      44.8   4.85e-42\n2 Temp^1       -0.318     0.0630     -5.04  6.36e- 6\n3 Temp^2       -0.00847   0.00662    -1.28  2.07e- 1\n4 After        -2.26      0.220     -10.3   6.52e-14\n5 After:Temp^1  0.180     0.0964      1.86  6.82e- 2\n6 After:Temp^2 -0.00651   0.00997    -0.653 5.17e- 1\n```\n\n\n:::\n:::\n\n\n\n\n### Simulate random data conforming to GLM with fixed design\n\nGenerate $N = 1000$ instances of the Gaussian Linear Model defined by \n$$\n\\begin{bmatrix} \\vdots \\\\\nY \\\\ \\vdots \\end{bmatrix}\n = \\begin{bmatrix} \\mbox{} & & \\mbox{} \\\\\n & X & \\\\\n &   & \\end{bmatrix} \\times \\widehat{\\beta} + \\widehat{\\sigma} \\times \\begin{bmatrix} \\vdots \\\\\n\\epsilon\\\\ \\vdots \\end{bmatrix}\n$$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\text{Id}_{56})$ and $X, \\widehat{\\beta}$, and $\\widehat{\\sigma}$ have been extracted above from the linear fit to the `whiteside` data.   \n\n::: {.callout-caution}\n\nTry to avoid unnecessary computations.\n\n:::\n\n\n::: {.content-visible when-profile='solution'} \n \n \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n:::\n\n\nFor each simulated instance, fit a linear model.  \n\n::: {.content-visible when-profile='solution'} \n \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n \n:::\n\n\nNow you should have $N$ identically distributed, independent realizations of the response vector $Y$ (and with some more work, realizations of prediction vectors $\\widehat{Y}$).   Denote the $N$ independent realizations of the response vectors by $Y^*_1, \\ldots, Y^*_N$, and denote the $N$ realizations of the estimators $\\widehat{\\beta}^*_1, \\ldots, \\beta^*_{N}$ and $\\widehat{\\sigma}^*_1, \\ldots, \\widehat{\\sigma}^*_N$. Use the same style of notation for predictions and residuals. \n\nThe  [Statistical Inference course](https://statsfonda.github.io/site/) tells us a lot of things about the distribution of the response vectors, the prediction vectors, the residuals,  and so on. Those theoretical results are used by function `lm()`, method `summary.lm()`, and diagnostic plot methods `plot.lm()` (and also by `aov()`, `anova()`, `stepAIC()`, ...)\n\n### Distribution of estimators of noise variance\n\nPlot your sample of estimators of the noise variance $\\widehat{\\sigma}^*_1, \\ldots, \\widehat{\\sigma}^*_N$. Compare with the theoretical density of the distribution of these estimators (histograms, CDF, quantile plots). \n\nFor $\\alpha=5\\%, 1\\%$, compute the $N$ confidence regions for $\\beta$ ($N$ ellipsoids), and compute the empirical *coverage* of your confidence regions (the number of times the true parameter belongs to the confidence region). How should this empirical coverage be distributed? What is its expectation? its departure from expectation? \n\n\n::: {.content-visible when-profile='solution'} \n \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n \n:::\n\n### Fluctuations of coefficients estimates (I)\n\nAccording to the GLM, the distribution of the coefficients estimates $\\widehat{\\beta}^*_1, \\ldots, \\widehat{\\beta}^*_N$ is known if the noise variance is known. \n\nVisualize  the empirical joint distribution of $(\\widehat{\\beta}^*_i[1], \\widehat{\\beta}^*_i[2])$. Compare with theoretical distribution.   \n\n\n::: {.content-visible when-profile='solution'} \n \n \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n:::\n\n### Fluctuations of coefficients estimates (II) : Studentized statistics\n\nIf the noise variance is not known, according to the GLM theory, we can use the noise variance estimator to build confidence regions. \n\nInvestigate and illustrate (histograms, CDF and quantiles plots) the distribution of the coefficients $\\frac{1}{\\widehat{\\sigma}}A \\times (\\widehat{\\beta}^*-\\widehat{\\beta})$ where $A$ is a well-chosen matrix (that may depend on the design). \n\n::: {.content-visible when-profile='solution'} \n \n \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n:::\n\n\n### Regression of $\\widehat{\\beta}^*[6]$ with respect to all other estimated coefficients $\\widehat{\\beta}^*[1,..,5]$\n\nThe sample of $N$ realizations of $\\widehat{\\beta}^*$ : $\\widehat{\\beta}^*_1, \\ldots, \\widehat{\\beta}^*_N$\nmay be considered as an instance of linear regression with respect to a *random design* where the response variable is $\\widehat{\\beta}^*[6]$  while the explanatory variables are $\\widehat{\\beta}^*[1], \\ldots, \\widehat{\\beta}^*[5]$.\n\nCompute the optimal regression coefficients. What is the distribution of $\\widehat{\\beta}^*[6] - \\mathbb{E}\\left[ \\widehat{\\beta}^*[6] \\mid \\widehat{\\beta}^*[1], \\ldots, \\widehat{\\beta}^*[5] \\right]$? Investigate graphically. \n\n\n### Diagnostic plots when the GLM assumptions hold\n\nPick $1$ linear fit amongst the $N$ linear fits performed on the simulated data. Draw the four diagnostic plots. Comment (briefly). \n\n\n### Overparametrized model\n\nDefine $\\widehat{\\theta} \\in \\mathbb{R}^6$ by zeroing the coefficients of $\\widehat{\\beta}$  corresponding to the quadratic terms (with respect to `Temp`)\n\n::: {.content-visible when-profile='solution'} \n \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n \n:::\n\n\nGenerate $N = 1000$ instances of the Gaussian Linear Model defined by \n$$\n\\begin{bmatrix} \\vdots \\\\\nY \\\\ \\vdots \\end{bmatrix}\n = \\begin{bmatrix} \\mbox{} & & \\mbox{} \\\\\n & X & \\\\\n &   & \\end{bmatrix} \\times \\widehat{\\theta} + \\widehat{\\sigma} \\times \\begin{bmatrix} \\vdots \\\\\n\\epsilon\\\\ \\vdots \\end{bmatrix}\n$$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\text{Id}_{56})$.  \n\n\n::: {.content-visible when-profile='solution'} \n \n \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY <- X %*% theta\nmat_Y <- matrix(rep(Y, N) + rnorm(N * n, mean = 0, sd = sgm), nrow = n, ncol = N)\nmat_coeffs <- pinvX %*% mat_Y\nmat_fitted <- H %*% mat_Y\nmat_resid <- mat_Y - mat_fitted\nsigma2_est <- apply(mat_resid^2, MARGIN = 2, sum) / (n - length(theta))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nZ <- X[, -c(3,6)]\nqrZ <- qr(Z)\nrZ <- qr.R(qrZ)\nqZ <- qr.Q(qrZ)\nHZ <- qZ %*% t(qZ)\nInvrZ <- solve(rZ)\nCovBetasZ <- InvrZ %*% t(InvrZ)\npinvZ <- InvrZ %*% t(qZ)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmat_coeffs_Z <- pinvZ %*% mat_Y\nmat_fitted_Z <- HZ %*% mat_Y\nmat_resid_Z <- mat_Y - mat_fitted_Z\nsigma2_est_Z <- apply(mat_resid_Z^2, MARGIN = 2, sum) / (n - length(theta)-2)\nF_stats <- (apply((mat_resid_Z - mat_resid)^2, MARGIN = 2, sum) / 2) / sigma2_est\n\n\n### summary(F_stats)\n### qf(c(1,2,3)/4,df1=2, df2=50)\n```\n:::\n\n\n\n\n:::\n\n\n### Estimators of noise variance\n\n\nFit all $N$ realizations  with the same formula as above. Compute the new estimators of the noise variance. \n\n::: {.content-visible when-profile='solution'} \n \n \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n:::\n\n### Student's tests for coefficients\n\nPerform student's tests for the coefficient vectors. How many times do you reject the null hypothesis  concerning the  coefficients of $\\widehat{\\beta}$  corresponding to the quadratic terms (with respect to `Temp`) if you choose a size/level equal\nto $5\\%$. Plot the sample of the $|t|$ values for the coefficients of $\\widehat{\\beta}$  corresponding to the quadratic terms (with respect to `Temp`).\n\nPlot the histogram of the empirical distrinbution of $p$-values. Compare with theoretical distribution of $p$ values.  \n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n### Fisher's test(s)\n\n\nWe aim at testing \n\n- $H_0$ : $\\widehat{\\theta}[3] = \\widehat{\\theta}[6] = 0$\n(null hypothesis, assuming that the third and the sixth coefficients represent `Temp^2` and `Temp^2:InsulAfter`)\n\nversus \n\n- $H_1$ : $\\widehat{\\theta}[3] \\neq 0 \\quad \\text{or} \\quad \\widehat{\\theta}[6] \\neq 0$ (alternative)\n\nCompute the Fisher statistics for the $N$ simulated response vectors (when the null hypothesis is true). Plot the Fisher statistics and compare to the theoretical distribution under the the null hypothesis. If you choose a level/size equal to $1\\%$, how many times do you reject the null hypothesis?\n\nCompute the Fisher statistics for the $N$ simulated response vectors (when the null hypothesis is not true). Plot the Fisher statistics and compare to the theoretical distribution under the the null hypothesis. If you choose a level/size equal to $1\\%$, how many times do you reject the null hypothesis?\n\n\nAgain plot a histogram of the empirical distribution of $p$-values\n\n::: {.content-visible when-profile='solution'} \n \n \n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n<!-- # Log-likelihood function -->\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n:::\n\n### Performance of `stepAIC`\n\nRun `stepAIC()` on the overparametrized models you obtain. \nDescribe graphically the distribution of outcomes, and the distribution of the AIC criteria for the selected models. \n\n\n### Departing from the Gaussian Linear Model assumptions\n\nReplay the above described simulations but replace the Gaussian noise with Student's noise with three degrees of freedom  `rt(N, df=3, ncp=0)` (with and without overparametrizatio). Recompute the Fisher statistics for testing $H_0$ against $H_1$. \nVisualize the distributions compare to the theoretical distribution of the Fisher statistics. If you choose a level/size equal to $1\\%$, how many times do you reject the null hypothesis?\n\n\n\n### References \n\n\n- [Poly. S. Boucheron](https://stephane-v-boucheron.fr/files/stats/notes-17-18.pdf)\n- [Poly. S. Coste](https://statsfonda.github.io/site/)\n\n### {{<  fa graduation-cap >}} Grading criteria \n\n\n| Criterion | Points  | Details |\n|:----------|:-------:|:--------|\n|Spelling and syntax | 20% | English/French {{<  fa pen-fancy >}}|\n|Plots correction | 25% | choice of `aesthetics`, `geom`, `scale` ... {{<  fa chart-area >}}|\n|Computing Statistics | 30% | Aggregations, LR, PCA, CA, ... {{<  fa chart-area >}} |\n|DRY compliance | 25% | DRY principle at {{<  fa brands wikipedia-w  >}} [ Wikipedia ](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)|\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "842d5c57cfb686f88d42fe81ba4341c5",
  "result": {
    "engine": "knitr",
    "markdown": "--- \ntitle: \"Hierarchical Clustering\"\ncategories: [Hierarchical clustering]\ndate: \"2025-04-02 (updated: 2025-04-03)\"\n\nexecute:\n  echo: false\n  eval: true\n\nformat: \n  revealjs:\n    header: \"Hierarchical Clustering\"\n\nengine: knitr\n--- \n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Hierarchical clustering {background-color=\"#1c191c\"}\n\n\n::: {.notes}\n\n- Dendrogram : what is it?\n- From dendrogram to clusterings: cutting a dendrogram\n- Grow a dendrogram\n- Displaying a dendrogram\n- Validation and assesment\n\n:::\n\n##\n\n> Hierarchical clustering [...] is a method of cluster analysis which seeks to build a hierarchy of clusters \n> \n> from Wikipedia\n\n. . .\n\nRecall that a clustering is a _partition_ of some dataset\n\nA partition $D$ of $\\mathcal{X}$ is a _refinement_ of another partition $D'$\nif every class in $D$ is a subset of a class in $D'$. Partitions $D$ and $D'$ are\nsaid to be _nested_\n\n. . .\n\nA hierarchical clustering of $\\mathcal{X}$ is a sequence of  $|\\mathcal{X}|$ nested partitions  of $\\mathcal{X}$, starting from the trivial partition into  $|\\mathcal{X}|$ singletons and ending  into the trivial partition in $1$ subset ( $\\mathcal{X}$ itself)\n\nA hierarchical clustering consists of $|\\mathcal{X}|$ nested flat clusterings\n\nWe will explore _agglomerative_ or _bottom-up_ methods to build hierarchical clusterings\n\n\n\n## Hierchical clustering and dendrogram  {.smaller}\n\n\n::: {columns}\n\n::: {.column width=\"35%\"}\n\nThe result of hierarchical clustering is a _tree_ where _leafs_ are labelled by sample points and\ninternal nodes correspond to merging operations\n\nThe tree conveys more information: if the tree is properly decorated, it is possible to reconstruct the different merging steps and to know which rule was applied when some merging operation was performed\n\nThe tree is called a _dendrogram_\n\n:::\n\n::: {.column}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/unnamed-chunk-3-1.png){width=768}\n:::\n:::\n\n\n\n```\nViolent Crime Rates by US State\n\nDescription\n\nThis data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.\n```\n\n:::\n\n:::\n\n##  {{< fa hand-point-right >}}\n\nDendrogram and trees show up in several areas.\n\nClassification and Regression trees play an important role in Machine Learning.\n\n`ggdendro` and `dendextend`  may also be used to manipulate regression trees\n\n\n\n\n\n\n##  {{< fa route >}}\n\n- Cutting a dendrogram: getting a flat clustering\n\n- Building a dendrogram: inside `hclust`\n\n- Displaying, reporting dendrograms\n\n\n\n## Cutting a dendrogram: Iris illustration  {.smaller}\n\n\n\n> The famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\n\n\n> The Iris flower data set is fun for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning.\n\n> The Setosa species are distinctly different from Versicolor and Virginica (they have lower petal length and width). But Versicolor and Virginica cannot easily be separated based on measurements of their sepal and petal width/length.\n\n\n## \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/iris-hclust-dendro-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ScaleContinuousPosition>\n Range:  \n Limits:    0 --    1\n```\n\n\n:::\n:::\n\n\n\n\n\n##\n\n- `as.matrix(dist(iris[,1:4]))` returns the matrix of pairwise distances\n- Default distance is Euclidean distance\n- What about using `broom::augment`?\n- There is no `augment.hclust` method: `No augment method for objects of class hclust`\n\n\n\n::: {.cell}\n\n:::\n\n\n\n:::\n\n\n## `hclust` pipeline   {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndendro_iris <- iris |> \n  select(where(is.numeric)) |>  \n  dist() |> \n  hclust() |> \n  dendro_data() \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmydendro <- . %$% {  #<<\n  ggplot() + \n    geom_segment(\n      data = segments,   #<<\n      aes(\n        x = x, y = y, \n        xend = xend, \n        yend = yend)\n  ) +\n  geom_text(\n    data = labels, #<<\n    aes(\n      x = x, y = y, \n      label = label, hjust = 0),\n    size = 2\n  ) +\n  coord_flip() +\n  scale_y_reverse(expand = c(0.2, 0)) +\n  theme_dendro()  \n}\n```\n:::\n\n\n\n\n\n##\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmydendro(dendro_iris)\n```\n\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n##  {.smaller}\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell output-location='column'}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Inside ggdendro\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n##\n\nWhat is an object of class `dendro` made of?\n\nIt a list of four elements:\n\n- `segments`\n- `labels`\n- `leaf_labels`\n- `class`\n\nElement `segments` is a data frame with four columns. Each row represent a segment that is part of a graphical representation of the hierarchy. There are horizontal and vertical segments \n\nElement `labels` is used to label the tree leafs. \n\n\n\n##\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## Cutting a dendrogram: {{< fa syringe >}} Iris illustration (continued)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- iris %>%\n  ggplot() +\n  aes(x=Petal.Length, y=Petal.Width)\n\np +\n  geom_point(\n    aes(\n      shape=Species, \n      colour=Species)\n  ) +\n# labs(shape= \"Species\") +\n  ggtitle(\n    label= \"Iris data\",\n    subtitle = \"Species in Petal plane\"\n  )\n```\n\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/iris-hclust-1-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## \n\nDoes the flat clustering obtained by cutting the dendrogram at some height\nreflect the partition into species?\n\n\n\n\n\n## Cutting a dendrogram:  Iris illustration (continued)\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/iris-hclust-2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Cutting a dendrogram:  Iris illustration (continued)\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/iris-hclust-3-1.png){width=960}\n:::\n:::\n\n\n\n## Cutting a dendrogram:  Iris illustration (continued)\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/iris-hclust-4-1.png){width=960}\n:::\n:::\n\n\n\n\n## Cutting a dendrogram:  Iris illustration (continued)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/iris-hclust-5-1.png){width=960}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Cutting a dendrogram:  better Iris illustration (continued)\n\n\nThe [`dendextend`](https://talgalili.github.io/dendextend/articles/dendextend.html) package offers a set of functions for extending _dendrogram_ objects in `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 581 512\" style=\"height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z\"/></svg>`{=html}, letting you\n\n- visualize and\n- compare trees of hierarchical clusterings,\n\nFeatures:\n\n- Adjust a tree’s graphical parameters - the color, size, type, etc, of its branches, nodes and labels\n- Visually and statistically compare different dendrograms to one another\n\n\n##\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/hang-iris-1.png){width=960}\n:::\n:::\n\n\n\n\n::: {.notes}\n\n- [dendextend](https://talgalili.github.io/dendextend/articles/dendextend.html)\n\n- [A dendro gallery](https://www.r-graph-gallery.com/340-custom-your-dendrogram-with-dendextend.html)\n\n- class of `dend`\n\n:::\n\n\n# Inside `hclust`   {background-color=\"#1c191c\"}\n\n\n## About class `hclust`\n\nResults from function `hclust()` are objects of class `hclust` :\n\n`iris_hclust` is an object of class hclust\n\nFunction `cutree()` returns a flat clustering of the dataset\n\n\n::: {.notes}\n\nWhat does `height` stand for?\n\nWhat does `merge` stand for?\n\nWhat does `order` stand for?\n\nHow different are the different `method`?\n\n:::\n\n## Hierarchical clustering of `USArrests`\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50\nColumns: 4\n$ Murder   <dbl> 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  <int> 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop <int> 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     <dbl> 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/dendro-USArrests-1.png){width=960}\n:::\n:::\n\n\n\n\n## About dendrograms (output `dendro_data()`)\n\nAn object of class `dendro` is a list of 4 objects:\n\n  - `segments`\n  - `labels`\n  - `leaf_labels`\n  - `class`\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Questions\n\n- How to build the dendrogram?\n\n- How to choose the cut?\n\n\n\n\n## Bird-eye view at hierarchical agglomerative clustering methods\n\n### All [hierarchical agglomerative clustering methods (HACMs)](https://en.wikipedia.org/wiki/Hierarchical_clustering) can be described by the following general algorithm.\n\n1. At each stage distances between clusters are recomputed by the _Lance-Williams dissimilarity update formula_ according to the particular clustering method being used.\n\n1. Identify the 2 closest _points_ and combine them into a cluster (treating existing clusters as points too)\n\n1. If more than one cluster remains, return to step 1.\n\n\n\n\n\n##  Greed is good!\n\n\n- Hierarchical agglomerative clustering methods are examples of _greedy algorithms_\n\n- Greedy algorithms sometimes compute _optimal solutions_\n\n  + Huffmann coding (Information Theory)\n\n  + Minimum spanning tree (Graph algorithms)\n\n- Greedy algorithms sometimes compute _sub-optimal solutions_\n\n  + Set cover (NP-hard problem)\n\n  + ...\n\n- Efficient greedy algorithms rely on ad hoc data structures\n\n  + Priority queues\n\n  + Union-Find\n\n\n\n\n\n\n## {{< fa binoculars >}} Algorithm (detailed)\n\n- Start with $(\\mathcal{C}_{i}^{(0)})= (\\{ \\vec{X}_i \\})$ the collection\n  of all singletons.\n\n- At step $s$, we have $n-s$ clusters $(\\mathcal{C}_{i}^{(s)})$:\n\n  -   Find the two most similar clusters according to a criterion\n        $\\Delta$: $$(i,i') = \\operatorname{argmin}_{(j,j')} \\Delta(\\mathcal{C}_{j}^{(s)},\\mathcal{C}_{j'}^{(s)})$$\n\n  -   Merge $\\mathcal{C}_{i}^{(s)}$ and $\\mathcal{C}_{i'}^{(s)}$ into\n        $\\mathcal{C}_{i}^{(s+1)}$\n\n  -   Keep the $n-s-2$ other clusters\n        $\\mathcal{C}_{i''}^{(s+1)} = \\mathcal{C}_{i''}^{(s)}$\n\n-   Repeat until there is only one cluster left\n\n\n\n## Analysis\n\n-   Complexity: $O(n^3)$ in general.\n\n-   Can be reduced to $O(n^2)$ (sometimes to $O(n \\log n)$)\n\n    -   if the number of possible mergers for a given cluster is  bounded.\n\n    -   for the most classical distances by maintaining a nearest\n        neighbors list.\n\n\n\n\n## Merging criterion based on the distance between points\n\n### Minimum linkage:\n$$\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) =\\min_{\\vec{X}_i \\in \\mathcal{C}_i} \\min_{\\vec{X}_j \\in    \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)$$\n\n### Maximum linkage:\n$$\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\max_{\\vec{X}_i \\in \\mathcal{C}_i} \\max_{\\vec{X}_j \\in    \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)$$\n\n### Average linkage:\n$$\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) =\\frac{1}{|\\mathcal{C}_i||\\mathcal{C}_j|} \\sum_{\\vec{X}_i \\in    \\mathcal{C}_i}\\sum_{\\vec{X}_j \\in \\mathcal{C}_j} d(\\vec{X}_i, \\vec{X}_j)$$\n\n\n\n##  Ward's criterion : minimum variance/inertia criterion\n\n\n$\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\sum_{\\vec{X}_i \\in \\mathcal{C}_i} \\left( d^2(\\vec{X}_i, \\mu_{\\mathcal{C}_i \\cup \\mathcal{C}_j} ) - d^2(\\vec{X}_i, \\mu_{\\mathcal{C}_i}) \\right) +$\n\n$\\qquad\\qquad \\qquad \\sum_{\\vec{X}_j \\in \\mathcal{C}_j} \\left( d^2(\\vec{X}_j, \\mu_{\\mathcal{C}_i \\cup \\mathcal{C}_j} ) - d^2(\\vec{X}_j, \\mu_{\\mathcal{C}_j}) \\right)$\n\n\n\n### If $d$ is the euclidean distance\n\n\n$$\\Delta(\\mathcal{C}_i, \\mathcal{C}_j) = \\frac{ |\\mathcal{C}_i||\\mathcal{C}_j|}{|\\mathcal{C}_i|+ |\\mathcal{C}_j|} d^2(\\mu_{\\mathcal{C}_i}, \\mu_{\\mathcal{C}_j})$$\n\n\n\n\n## Lance-Williams update formulae\n\nSuppose that clusters $C_{i}$ and $C_{j}$ were next to be merged\n\nAt this point, all of the current pairwise cluster _distances_ are known\n\nThe recursive update formula gives the updated cluster _distances_ following the pending merge of clusters $C_{i}$ and $C_{j}$\n\nLet\n\n- $d_{ij}, d_{ik}$, and $d_{jk}$ be shortands for the pairwise _distances_ between clusters $C_{i}, C_{j}$ and $C_{k}$\n\n- $d_{{(ij)k}}$ be shortand  for the _distance_ between the new cluster $C_{i}\\cup C_{j}$ and $C_{k}$ ( $k\\not\\in \\{i,j\\}$ )\n\n\n\n## Lance-Williams update formulae (continued)\n\nAn algorithm belongs to the _Lance-Williams family_ if the updated cluster _distance_ $d_{{(ij)k}}$ can be computed recursively by\n\n$$d_{(ij)k} = \\alpha _{i}d_{ik}+ \\alpha _{j}d_{jk}+ \\beta d_{ij}+ \\gamma |d_{ik}-d_{jk}|$$\n\nwhere $\\alpha_{i},\\alpha _{j},\\beta$ , and $\\gamma$  are parameters, which may depend on cluster sizes, that together with the cluster _distance_ function $d_{ij}$ determine the clustering algorithm.\n\n::: {.notes}\n\nClustering algorithms such as \n\n- single linkage, \n- complete linkage, and \n- group average \n\nmethod have a recursive formula of the above type\n\n\n:::\n\n## Lance-Williams update formula for Ward's criterion\n\n$$\\begin{array}{rl}d\\left(C_i \\cup C_j, C_k\\right) & = \\frac{n_i+n_k}{n_i+n_j+n_k}d\\left(C_i, C_k\\right)  +\\frac{n_j+n_k}{n_i+n_j+n_k}d\\left(C_j, C_k\\right) \\\\ & \\phantom{==}- \\frac{n_k}{n_i+n_j+n_k} d\\left(C_i, C_j\\right)\\end{array}$$\n\n$$\\alpha_i = \\frac{n_i+n_k}{n_i+n_j+n_k} \\qquad \\alpha_j = \\frac{n_j+n_k}{n_i+n_j+n_k}\\qquad \\beta = \\frac{- n_k}{n_i+n_j+n_k}$$\n\n\n\n## An unfair quotation\n\n> Ward's minimum variance criterion minimizes the total within-cluster variance .fr[Wikipedia]\n\n\n- Is that correct?\n\n- If corrected, what does it mean?\n\n\n::: {.notes}\n\nIf we understand the statement as:\n\n> for any $k$, the flat clustering obtained by cutting the dendrogram so as to obtain a $k$-clusters partition  minimizes the total within-cluster variance/inertia  amongst all $k$-clusterings\n\nthen, the statement is not proved. If it were proved, it would  imply $\\mathsf{P}=\\mathsf{NP}$\n\nThe total within-cluster variance/inertia is the objective function in the $k$-means problem.\n\nThe statement is misleading\n\n:::\n\n\n## What happens in Ward's method?\n\n> At each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging .fr[Wikipedia]\n\n> This increase is a weighted squared distance between cluster centers .fr[Wikipedia]\n\n> At the initial step, all clusters are singletons (clusters containing a single point). To apply a recursive algorithm under this objective function, the initial distance between individual objects must be (proportional to) squared Euclidean distance.\n\n\n\n\n## Views on Inertia:\n\n$$I   = \\frac{1}{n} \\sum_{i=1}^n \\|\\vec{X}_i - \\vec{m} \\|^2$$\n\nwhere $\\vec{m} = \\sum_{i=1}^n \\frac{1}{n}\\vec{X}_i$\n\n$$I = \\frac{1}{2n^2} \\sum_{i,j} \\|\\vec{X}_i - \\vec{X}_j\\|^2$$\n\n\nTwice the  mean squared distance to the mean equals the  mean squared distance between sample points\n\n::: {.notes}\n\nRecall that for a real random variable $Z$ with mean $\\mu$\n\n$$\\operatorname{var}(Z) = \\mathbb{E}(Z -m)^2 = \\inf_a \\mathbb{E}(Z -a)^2$$\n\nand\n\n$$\\operatorname{var}(Z) = \\frac{1}{2} \\mathbb{E}(Z -Z')^2$$\n\nwhere $Z'$ is an independent copy of $Z$\n\nThe different formulae for inertia is just mirroring the different views at variance\n\nThe inertia is the trace of an empirical covariance matrix.\n\n:::\n\n\n\n## Decompositions of inertia (Huyghens formula)\n\n- Sample  $x_1,\\ldots, x_{n+m}$ with mean $\\bar{X}_{n+m}$  and  variance $V$\n\n- Partition  $\\{1,\\ldots,n+m\\} = A \\cup B$  with   $|A|=n, |B|=m$, $A \\cap B =\\emptyset$\n\n- Let $\\bar{X}_n = \\frac{1}{n}\\sum_{i \\in A} X_i$ and $\\bar{X}_m=\\frac{1}{m}\\sum_{i \\in B}X_i$\n$$\\bar{X}_{n+m} =  \\frac{n}{n+m} \\bar{X}_{n}  +\\frac{m}{n+m} \\bar{X}_{m}$$\n\n- Let $V_A$  be the variance of $(x_i)_{i\\in A}$, $V_B$ be the  variance of $(x_i)_{i\\in B}$\n\n\n\n## Decompositions of inertia (Huyghens formula)\n\n- Let $V_{\\text{between}}$ be the  variance of a ghost sample with   $n$ copies of $\\bar{X}_n$ and $m$ copies of $\\bar{X}_m$\n$$V_{\\text{between}} =  \\frac{n}{n+m} (\\bar{X}_n -\\bar{X}_{n+m})^2 + \\frac{m}{n+m} (\\bar{X}_m -\\bar{X}_{n+m})^2$$\n\n- Let $V_{\\text{within}}$ be the weighted mean of variances within classes $A$ and $B$\n$$V_{\\text{within}}  = \\frac{n}{n+m}  V_A + \\frac{m}{n+m} V_B$$\n\n\n\n## Decompositions of inertia\n\n::: {.callout-important}\n\n### Proposition:  Huyghens formula I\n\n$$V = V_{\\text{within}} +  V_{\\text{between}}$$\n\n:::\n\n\n## Huyghens formula can be extended to any number of classes\n\n::: {.callout-important}\n\n### Proposition: Huyghens (II)\n\n- Sample  $\\vec{x}_1, \\ldots,\\vec{x}_n$  from $\\mathbb{R}^p$ with mean $\\bar{X}_n$, inertia  $I$.\n\n- Let $A_1, A_2\\ldots, A_k$ be a partition of\n$\\{1,\\ldots,n\\}$.\n\n- Let $I_\\ell$ (resp. $\\bar{X}^\\ell$) be the inertia (resp. the mean) of sub-sample $\\vec{x}_i, i\\in A_\\ell$\n\n- Let $I_{\\text{between}}$ be the inertia of the ghost sample, formed by  $|A_1|$ copies of $\\bar{X}^1$,  $|A_2|$ copies of $\\bar{X}^2$,  ...\n $|A_k|$ copies of $\\bar{X}^k$\n\n- Let $I_{\\text{within}} =  \\sum_{\\ell=1}^k \\frac{|A_\\ell|}{n}  I_\\ell$\n\n$$I =   I_{\\text{within}} +  I_{\\text{between}}$$\n\n:::\n\n\n\n\n# Comparing dendrograms  {background-color=\"#1c191c\"}\n\n\n## Cophenetic disimilarity\n\nGiven a dendrogram, the _cophenetic_ disimilarity between two sample points $x, x'$ is\nthe intergroup disimilarity at which observations $x$ and $x'$ are first joined.\n\n::: {.callout-important}\n\n### Proposition\n\nA cophenetic disimilarity has the _ultrametric_ property\n\n:::\n\n\n::: {.aside}\n\nAll triangles are isoceles and the unequal length should be no longer than the length of the two equal sides\n\n:::\n\n\n\n## Cophenetic correlation coefficient\n\n\nThe cophenetic correlation coefficient measures  how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points \n\n::: {.aside}\n[wikipedia](https://en.wikipedia.org/wiki/Cophenetic_correlation)]\n\n:::\n\n## Computing cophenetic correlation coefficient\n\nIn  {{< fa brands r-project >}} use the `dendextend` package\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| single| complete| average| mcquitty| ward.D| centroid| median| ward.D2|\n|------:|--------:|-------:|--------:|------:|--------:|------:|-------:|\n|   0.86|     0.73|    0.88|     0.87|   0.86|     0.87|   0.86|    0.87|\n\n\n:::\n:::\n\n\n\n##\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-hclust_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## How to apply general algorithm?\n\n- Lance-Williams dissimilarity update formula calculates dissimilarities between a new cluster and existing points, based on the dissimilarities prior to forming the new cluster\n\n- This formula has 3 parameters\n\n- Each HACM is characterized by its own set of Lance-Williams parameters\n\n\n\n## Implementations of the general algorithm\n\n\n### Stored matrix approach\n\nUse matrix, and then apply Lance-Williams to recalculate dissimilarities between cluster centers. Storage  $O(N^2)$ and time at least $O(N^2)$, but is $\\Theta(N^3)$ if matrix is scanned linearly\n\n### Stored data approach\n\n$O(N)$ space for data but recompute pairwise dissimilarities, needs $\\Theta(N^3)$ time\n\n\n### Sorted matrix approach\n\n$O(N^2)$ to calculate dissimilarity matrix, $O(N^2 \\log N)$ to sort it, $O(N^2)$ to construct hierarchy, but one need not store the data set, and the matrix can be processed linearly, which reduces disk accesses\n\n\n\n## Agglomerative Clustering Heuristic\n\n\n-   Start with very small clusters (a sample point by cluster?)\n\n-   Merge iteratively the most similar clusters according to some *greedy* criterion $\\Delta$.\n\n-   Generates a _hierarchy of clusterings_ instead of a single one.\n\n-   Need to select the number of cluster afterwards.\n\n-   Several choice for the merging criterion\n\n-   Examples:\n\n    -   _Minimum Linkage_: merge the closest cluster in term of the usual\n        distance\n\n    -   _Ward's criterion_: merge the two clusters yielding the less inner\n        inertia loss (minimum variance criterion)\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Packages\n\n- `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 581 512\" style=\"height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z\"/></svg>`{=html}\n  + [`ggdendro`](https://cran.r-project.org/web/packages/ggdendro/vignettes/ggdendro.html)\n  + [`dendextend`](https://cran.r-project.org/web/packages/dendextend/vignettes/dendextend.html)\n  + [`dendroextras`](https://cran.r-project.org/web/packages/dendroextras/index.html)\n\n- `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 448 512\" style=\"height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z\"/></svg>`{=html}\n  + [`scipy`](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)\n  + [`scikit-learn`](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n\n# The End   {background-color=\"#1c191c\"}",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}